[{"content":"В продолжении мы настроим оповещение в Zabbix и визуализацию в Grafana\nШаг 5. Настроить оповещение Добавляем адрес EMail SMTP сервера. Для этого переходим в:\nAdministration -\u0026gt; Media Types -\u0026gt; EMail\nУказываем настройки вашего SMTP сервера\nСоздаем группу:\nConfiguration -\u0026gt; User Groups -\u0026gt; Create\n1 2 Name: Alert only Permission: Read (Выдаём права на сервер, с которого собирается метрика, у меня Zabbix Server) Создаем пользователя, который будет получать уведомления:\nAdministration -\u0026gt; Users -\u0026gt; New\nВ Media добавляем sms, Email (HTML)\nПереходим в созданную нами ранее группу и добавляем нашего пользователя alert_user:\nConfiguration -\u0026gt; User Groups -\u0026gt; Alert only\n1 Users: -\u0026gt; Select alert_user -\u0026gt; Operations\n1 2 3 Default: 1h Send to user groups: alert_user Send only to: Email (HTML) Настраиваем действие (Action) на триггер:\nConfiguration -\u0026gt; Action -\u0026gt; Create Action\nДалее добавляем Operation:\nНастройка выполнена.\nМожно изменить в скрипте из первой части 0 и 1 местами и проверить весь процесс срабатывания триггера и пересчёта SLA.\nДополнительно\nПо желанию можно сделать отправку по Email (HTML). Кастомный шаблон.\nШаг 7. Визуализация в Grafana Предполагается, что к вашей Grafana уже подключен источник данных Zabbix. Здесь я это не затрагиваю.\nСоздаём новую панель:\nDashboard -\u0026gt; New Add a new panel\nВыбираем: Query\n1 2 3 4 5 6 7 8 9 10 11 12 13 Data Source: Zabbix Type: Stat Count A: - Query type: Triggers - Count by: All triggers - Group: Server - Min severity: Warning - Count: On Count B: - Query type: Services - Service: SITE - SLA: SITE - Property: SLI Далее переходим в: Transform\n1 2 3 4 5 6 7 8 9 Rename by regex: - Match: (Count A) - Replace: Status Rename by regex: - Match: (Count B) - Replace: Available Filter by name: - Count A - SLI SITE Настройка расположения и прочего - вкусовщина. Я делаю так:\nИтог:\nРаз в минуту zabbix-server будет обращаться к zabbix-agent для получения метрики. Сервер будет запускать скрипт, авторизовываться и возвращать значение параметра.\nПолученное значение отображается в Web-интерфейсе Zabbix, рассчитывается SLA. В Grafana выводится бизнес-метрика и значение в виде графика.\nЕсли авторизация не удается более 2-х раз, то срабатывает триггер, который отправляет по Email сообщение администратору.\n","date":"2025-10-30T12:00:00+05:00","image":"https://itzm.tech/p/zabbix2/zabbix2_hu8682595376433417234.jpg","permalink":"https://itzm.tech/p/zabbix2/","title":"Zabbix \u0026 Grafana: Мониторинг сайта скриптами. Часть 2"},{"content":"Дано: Критически важный сайт, который нужно мониторить и в случае неработоспособности уведомлять админов.\nВерсия Zabbix: 6.0.1 Версия Grafana: 11.6.0 Мониторинг будем настраивать на получение кода 200.\nЕсли авторизация удалась, то считаем это за 1, если нет, то 0.\nЗапрос каждую минуту 2-е неуспешные авторизации = срабатывание триггера В первой части статьи мы создадим скрипт и настроим zabbix на сбор метрик.\nВо второй части сделаем оповещение пользователей Zabbix и визуализируем результаты мониторинга в Grafana.\nПорядок действий:\nВыбрать сервер, с установленным zabbix-агентом. Написать скрипт. Добавить пользовательский параметр в конфигурацию zabbix-агента Создать шаблон (элементы, триггеры) + назначить хосту (В моём примере это будет сам zabbix-сервер) Настроить SLA Настроить оповещения Создать пользователя (группу пользователей) для уведомлений + создание действия Визуализация в Grafana Шаг 1. Выбрать сервер, с установленным zabbix-агентом. Написать скрипт. В качестве сервера, который будет запускать скрипт и получать значение у меня будет Zabbix-сервер. Захожу на сервер. Создаю скрипт:\n1 sudo nano /opt/monitoring/site-check.py Пример написанного мною скрипта. Используется Python3 и библиотеки: requests, sys, warnings.\nЛинк на Github\nДелаем исполняемым:\n1 sudo chmod +x /opt/monitoring/site-check.py Меняем группу владельца на zabbix\n1 sudo chown root:zabbix /opt/monitoring/site-check.py Запускаем тестируем:\n1 ./opt/monitoring/site-check.py Шаг 2. Добавить пользовательский параметр в конфигурацию zabbix-агента 1 sudo nano /etc/zabbix/zabbix_agentd.conf Находим параметр: UserParameter\nГде, site.auth - переменная, в которую будет записываться значение полученное после выполнения скрипта. /opt/monitoring/site-check.py - путь до скрипта.\n1 UserParameter=site.auth,/opt/monitoring/site-check.py Рестартим сервис:\n1 systemctl restart zabbix-agent.service Проверяем, что zabbix понимает переменную:\n1 zabbix_agentd -t site.auth Шаг 3. Создать шаблон (элементы, триггеры) + назначить хосту Идем в веб-интерфейс Zabbix. Создаём шаблон:\nTemplates -\u0026gt; Create template\n-\u0026gt; Add\nСоздаем элемент:\nItems -\u0026gt; Create item\nДалее Test -\u0026gt; Host address Наш сервер, на котором запускается скрипт -\u0026gt; Port: 10050\nВ Value появится 1, если авторизация доступна или 0 если недоступна.\n-\u0026gt; Add\nСоздаем триггер\nTriggers -\u0026gt; Create Trigger\nЯ закладываю логику: Если два раз подряд авторизация завершится неудачно, тогда срабатывает триггер.\nНапоминаю, что используется Zabbix 6.0 поэтому параметры типа count с тремя значениями использоваться ещё не могут. Будем брать последние 2 значения:\n1 Expression: last(/Template Auth Monitoring/site.auth,#1)=0 and last(/Template Auth Monitoring/site.auth,#2)=0 где, #1 = Последнее полученное значение ключа, #2 = Предпоследнее полученное значение ключа Далее переходим в Tag и назначаем\n-\u0026gt; Add\nШаг 4. Настройка SLA Service -\u0026gt; SLA -\u0026gt; Create SLA\n1 Reporting period\u0026#34; Quarterly # Считать статистику будем за квартал -\u0026gt; Add\nСоздаём сервис\nServices -\u0026gt; Edit - Create service\n-\u0026gt; Add\nНазначаем тег\nНастройка сбора метрик завершена. В следующей части оповещения и визуализация в Grafana.\nЧасть 2 -\u0026gt; https://itzm.tech/p/zabbix2/\n","date":"2025-10-30T11:45:00+05:00","image":"https://itzm.tech/p/zabbix1/zabbix1_hu11664412559662732595.png","permalink":"https://itzm.tech/p/zabbix1/","title":"Zabbix \u0026 Grafana: Мониторинг сайта скриптами. Часть 1"},{"content":"Redpanda 1 nano docker-compose.yaml Содержимое:\n1 2 3 4 5 6 7 8 9 10 11 version: \u0026#34;3.8\u0026#34; services: redpanda-console: image: docker.redpanda.com/redpandadata/console:latest container_name: redpanda-console ports: - \u0026#34;8080:8080\u0026#34; environment: KAFKA_BROKERS: \u0026#34;kafka:9092\u0026#34; # адрес твоего Kafka-брокера CONSOLE_LISTEN_ADDRESS: \u0026#34;0.0.0.0:8080\u0026#34; Заходим в браузере на IP:8080\nKafka UI 1 nano docker-compose.yaml Содержимое:\n1 2 3 4 5 6 7 8 9 10 11 version: \u0026#34;3.8\u0026#34; services: kafka-ui: image: provectuslabs/kafka-ui:latest container_name: kafka-ui ports: - \u0026#34;8080:8080\u0026#34; environment: KAFKA_CLUSTERS_0_NAME: \u0026#34;local\u0026#34; KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: \u0026#34;kafka:9092\u0026#34; # замени на адрес твоего брокера Заходим в браузере на IP:8080\n","date":"2025-10-03T17:16:00+05:00","image":"https://itzm.tech/p/kafka_ui/ui-kafka_hu10020923851716177774.jpg","permalink":"https://itzm.tech/p/kafka_ui/","title":"Веб-интерфейс для Kafka"},{"content":"В данном примере будет выполнена \u0026ldquo;Установка OpenSearch\u0026rdquo; используя официальную роль Ansible\nТеорию и многое другое доступно описываю в твоём ТГ канале.\nТам же отвечаю на вопросы и обновляю инструкции при необходимости.\nПрисоединяйся Требования Установленный сервер управления - Ansible.\nНастроенный пользователь для подключение ansible к хостам без пароля, с правами root.\nРесурсы нод:\nCPU: 2 RAM: 8 gb SSD: 100 gb OS: Ubuntu Установка Клонируем роль:\n1 git clone https://github.com/opensearch-project/ansible-playbook.git Заходим:\n1 cd ansible-playbook Добавляем хосты:\n1 nano inventories/opensearch/hosts В официальной документации 5 хостов, в примере 3:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 os-node-1 ansible_host=10.0.1.1 ansible_user=root ip=10.0.1.1 roles=data,master os-node-2 ansible_host=10.0.1.2 ansible_user=root ip=10.0.1.2 roles=data,master os-node-3 ansible_host=10.0.1.3 ansible_user=root ip=10.0.1.3 roles=data,master os-node-web ansible_host=10.0.1.4 ansible_user=root ip=10.0.1.4 # List all the nodes in the os cluster [os-cluster] os-node-1 os-node-2 os-node-3 # List all the Master eligible nodes under this group [master] os-node-1 os-node-2 os-node-3 [dashboards] os-node-web Открываем на редактирование:\n1 nano inventories/opensearch/group_vars/all/all.yml Далее изменяем переменные на свои:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Имя кластера os_cluster_name: my-cluster # На момент выхода заметки актуальная версия 3.0.0. Можно посмотреть на сайте opensearch os_version: \u0026#34;3.0.0\u0026#34; os_dashboards_version: \u0026#34;3.0.0\u0026#34; # Доменное имя domain_name: itzm.tech # Память в ГБ выделяемая Java для работы OpenSearch xms_value: 2 xmx_value: 2 # Если всего серверов в кластере 1, то параметр заменить на cluster_type: single-node cluster_type: multi-node # Включить, если в будущем планируется управлять кластером с помощью этой ansible-роли. # Чтобы не пересоздавать сертификаты каждый раз при запуске плейбука, данный параметр создаст их один раз и не будет изменять. # Полезно, так как в проде пересоздание сертификатов для Opensearch может повалить tls и весь кластер. iac_enable: yes Команда для запуска. Прежде чем запускать читаем описание.\n1 ansible-playbook -i inventories/opensearch/hosts opensearch.yml --extra-vars \u0026#34;admin_password=Test@123 kibanaserver_password=Test@6789 logstash_password=Test@456\u0026#34; --become Для admin требуется надёжный пароль, например myStrongPassword123!\nКластер не запустится со слабым паролем (например, admin) или без пароля.\nЧто это за пароли и пользователи?\nadmin_password - главный пользователь, имеет полный доступ в веб-интерфейсе OpenSearch Dashboards и по API. kibanaserver_password - используется внутренним сервисом OpenSearch Dashboards (вход от имени kibanaserver) для общения с OpenSearch. Не для пользователей. logstash_password - используется Logstash (или другое приложение, которое ты будешь подключать) для отправки логов в OpenSearch. Можно использовать дефолтную роль admin для поключения приложений типа Logstash. Но не забывайте про риски:\nНарушается принцип наименьших привилегий. Logstash получает доступ ко всему (в т.ч. и системным индексам, снапшотам, логам). Риск компрометации. Если Logstash будет взломан, злоумышленник получит полный доступ к OpenSearch. Сложнее отследить, кто именно выполняет действия, так как всё будет от имени admin. Теперь меняем пароли в команде запуска плейбука на свои, запускаем и начинаем установку.\nЗаходим в веб-интерфейс После всех действий попадаем на страницу dashboards.\nIP-dasboard:5600\nВ моём случае 10.0.1.4:5600\nПроверяем состояние кластера в DevTools командой:\n1 GET _cluster/health Балансировка нагрузки Хорошей практикой является настройка балансировки нагрузки и повышение отказоустойчивости при выходе ноды из строя.\nДля этого я установлю единую точку входа в лице Nginx. Делать это я буду на сервере с dashboards.\n1 apt install nginx -y Конфигурация nginx.conf:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 events { worker_connections 1024; } http { upstream backend { server os-node-1.itzm.tech:9200; server os-node-2.itzm.tech:9200; server os-node-3.itzm.tech:9200; keepalive 15; } server { listen 9200 ssl; server_name os-node-web.itzm.tech; client_max_body_size 100M; ssl_certificate /etc/nginx/ssl/os-cert.crt; ssl_certificate_key /etc/nginx/ssl/os-key.key; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5; location / { proxy_pass http://backend; proxy_read_timeout 5m; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } } Создать сертификаты. Самоподписанные или ЦА в директории /etc/nginx/ssl с именами os-cert.crt, os-key.key.\nЕсли используется авторизация по сертификатам внутри OpenSearch, то конфиг такой:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 events { worker_connections 1024; } http { upstream os-node-web.itzm.tech { server os-node-1.itzm.tech:9200; server os-node-2.itzm.tech:9200; server os-node-3.itzm.tech:9200; keepalive 15; } server { listen 9200 ssl; server_name os-node-web.itzm.tech; client_max_body_size 100M; ssl_certificate /etc/nginx/ssl/os-cert.crt; ssl_certificate_key /etc/nginx/ssl/os-key.key; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers HIGH:!aNULL:!MD5; location / { proxy_pass https://os-node-web.itzm.tech; proxy_ssl_certificate /etc/nginx/ssl/cert.pem; proxy_ssl_certificate_key /etc/nginx/ssl/certkey.key; proxy_ssl_trusted_certificate /etc/nginx/ssl/root-ca.crt; proxy_read_timeout 5m; proxy_ssl_verify on; proxy_ssl_server_name off; proxy_ssl_verify_depth 2; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } } Перезапускаем nginx и пользуемся:\n1 systemctl restart nginx.service ","date":"2025-09-05T12:55:00+05:00","image":"https://itzm.tech/p/install_opensearch/opensearch_hu5927020894892597000.jpg","permalink":"https://itzm.tech/p/install_opensearch/","title":"Установка Opensearch Ansible"},{"content":" Что такое Kafka, для чего используется и логика работы подробно расписана в моём телеграм канале. Присоединяйся Цель:\n3 Kafka-брокера (для отказоустойчивости) 3 контроллера (KRaft controller nodes) будем совмещать с брокерами, но на проде лучше выделить Требования Для прода, для теста делить ресурсы на / 2:\nCPU: 4 RAM: 16 GB Disk: SSD. Минимум 500 ГБ на брокера Сеть: 1 Gbps минимум Система: Ubuntu или CentOS Установленный ansible: 2.15 и выше Создаём директорию:\n1 mkdir -p roles/kafka Клонируем репозиторий:\n1 git clone https://github.com/dragomirr/ansible-role-kafka.git roles/kafka Создаём директорию с инвентари:\n1 2 mkdir -p roles/kafka/inventory nano roles/kafka/inventory/hosts.ini Наполняем:\n1 2 3 4 [kafka_cluster] server-1 ansible_host=192.168.10.11 ansible_user=root kafka_node_id=1 server-2 ansible_host=192.168.10.12 ansible_user=root kafka_node_id=2 server-3 ansible_host=192.168.10.13 ansible_user=root kafka_node_id=3 Создаём playbook:\n1 nano playbook.yaml Наполняем:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 - name: Install Kafka hosts: kafka_cluster become: yes roles: - role: kafka kafka_heap_size: 2G kafka_install_dependencies: true kafka_topics: - name: MyFirstTopic replication_factor: 1 partitions: 10 kafka_additional_config: message.max.bytes: 1048576 # 1 MiB kafka_opts: - -XX:NewSize=256m Структура каталогов: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ├── inventory │ └── hosts.ini ├── playbook.yaml └── roles └── kafka ├── CHANGELOG.md ├── defaults │ └── main.yml ├── handlers │ └── main.yml ├── meta │ └── main.yml ├── molecule │ ├── cluster │ │ ├── converge.yml │ │ ├── INSTALL.rst │ │ ├── molecule.yml │ │ └── verify.yml │ ├── cluster_combined │ │ ├── converge.yml │ │ ├── INSTALL.rst │ │ ├── molecule.yml │ │ └── verify.yml │ └── default │ ├── converge.yml │ ├── INSTALL.rst │ ├── molecule.yml │ └── verify.yml ├── README.md ├── tasks │ └── main.yml ├── templates │ ├── kafka.service.j2 │ └── server.properties.j2 └── vars └── main.yml Запускаем установку:\n1 ansible-playbook -i inventory/hosts.ini playbook.yaml Проверка установки Заходим на одну из нод по ssh и выполняем команду:\n1 /opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list Записываем сообщение в созданный топик (MyFirstTopic):\n1 echo \u0026#34;Hello, World from Kafka\u0026#34; | /opt/kafka/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic MyFirstTopic Читаем записанное сообщение:\n1 /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic MyFirstTopic --from-beginning Hello, World from Kafka\n","date":"2025-08-22T12:35:00+05:00","image":"https://itzm.tech/p/install_kafka/kafka_hu12035024302170000458.jpg","permalink":"https://itzm.tech/p/install_kafka/","title":"Установка кластера Apache Kafka через Ansible"},{"content":" Что такое MinIO, для чего нужен и как его использовать подробно расписал в моём телеграм канале.\nПоймёт даже новичок. Здесь только практика.\nТам же отвечаю на вопросы и обновляю инструкции при необходимости.\nПрисоединяйся Установка MinIO Установка из бинарного файла (Linux/macOS)\nВнимание, если установлен Midnight Commander (mc), то minio не сможет выполнять свою команду, которая также называется mc (MinIO Client).\nЗагрузка пакетов и установка:\n1 2 3 4 5 6 wget https://dl.min.io/server/minio/release/linux-amd64/minio wget https://dl.minio.io/client/mc/release/linux-amd64/mc chmod +x minio chmod +x mc sudo mv minio /usr/local/bin/ sudo mv mc /usr/bin/mc Создание пользователя под запуск службы и директорий:\n1 2 3 useradd -r minio -s /sbin/nologin mkdir -p /data/minio /etc/minio chown -R minio:minio /data/minio/ /etc/minio Открываем на редактирование файл с переменными:\n1 nano /etc/default/minio Добавляем в конфиг:\n1 2 3 4 MINIO_ROOT_USER=\u0026#34;admin\u0026#34; MINIO_ROOT_PASSWORD=\u0026#34;password\u0026#34; MINIO_VOLUMES=\u0026#34;/data/minio\u0026#34; MINIO_OPTS=\u0026#34;--console-address :9001\u0026#34; Создание сервиса Создаём сервис:\n1 nano /etc/systemd/system/minio.service Заполняем:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [Unit] Description=MinIO After=network.agent [Service] User=minio Group=minio EnvironmentFile=/etc/default/minio ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES Restart=always LimitNOFILE=65536 [Install] WantedBy=multi-user.target Перезапускаем и проверяем статус:\n1 2 3 systemctl daemon-reload systemctl enable --now minio.service systemctl status minio Делаем запись в DNS. Например, мой сервер будет открываться в вебе: myminio.s3.com\nГенерация сертификатов Создаём директорию:\n1 mkdir -p /etc/minio/certs И самоподписанный сертификат:\n1 2 3 4 openssl req -x509 -nodes -days 365 -newkey rsa:2048 \\ -keyout /etc/minio/certs/private.key \\ -out /etc/minio/certs/public.crt \\ -subj \u0026#34;/CN=myminio.s3.com\u0026#34; Добавляем сертификаты в конфигурацию:\n1 nano /etc/default/minio Добавляем строку:\n1 MINIO_OPTS=\u0026#34;--console-address :9001 --certs-dir /etc/minio/certs\u0026#34; Изменяем разрешения на сертификаты:\n1 2 3 chown -R minio:minio /etc/minio/certs chmod 600 /etc/minio/certs/private.key chmod 644 /etc/minio/certs/public.crt Перезагружаем службу:\n1 systemctl restart minio.service Вход Зайти в браузере:\n1 https://myminio.s3.com:9001/login Учетные данные: admin:password\nУстановка завершена.\n","date":"2025-08-07T10:30:00+05:00","image":"https://itzm.tech/p/minio_linux/minio_hu12633350659381903370.jpg","permalink":"https://itzm.tech/p/minio_linux/","title":"Установка MinIO в Ubuntu 22.04"},{"content":"Основные команды\nКоманда Описание ls Показать все файлы и директории в текущем каталоге ls -l Показать все файлы и директории в текущем каталоге с подробностями ls -t Вывод ls с сортировкой по дате последнего изменения pwd Полный путь к текущему каталогу cd directory Сменить директорию cd .. Перейти на одну директорию вверх cd / Перейти в корневой каталог clear Очистка терминала history Показать историю выполенных команд в текущем сеансе touch filename Создать новый файл vi filename Открыть файл для редактирования. Редактором vi cat filename Вывести содержимое файла в терминал hostnamectl Вывод системной информации. Имя хоста, ядро, архитектура, и прочее ifconfig Показать сетевые интерфейсы. IP-адреса и MAC-адреса системы date Вывод текущей системной даты и времени top Список всех запущенных процессов в системе free -m Показать статистику использования памяти в мегабайтах head filename Вывести первые 10 строк файла (голова) tail filename Вывести последние 10 строк файла (хвост) mv file /new/file/path Переместить файл в другое место mv old_filename new_filename Переименовать файл cp filename filename_2 Скопировать файл man command_name Просмотреть информацию о команде rm filename Удалить файл rm –rf directory_name Удалить каталог принудительно и его содержимое рекурсивно sudo Позволяет обычным пользователям запускать команды с повышенными привилегиями mkdir directory_name Создать новую директорию kill pid Убить процесс используя его ID reboot Перезагрузка системы shutdown –h now Выключение системы poweroff Выключение системы. Аналогична нажатию кнопки питания Сетевые команды\nКоманда Описание dig domain_name Показать информацию, связанную с DNS для данного доменного имени dig domain +short Вывести только основные данные host domain_name Выполнить DNS-поиск указанного доменного имени, который выведет IP-адрес whois domain_name Для получения дополнительной информации о домене ping ip Проверить соединение между хостом и указанным IP-адресом ssh username@ip Для SSH входа в систему под указанным пользователем на другом сервере wget file Скачать файл wget -c file Продолжить остановленную загрузку traceroute domain_name Отследить маршрут передачи пакета от текущего хоста к другому хосту telnet domain_name port Подключиться к удаленному хосту через определенный порт netstat –pnltu Отобразить все прослушиваемые в данный момент порты ss -tulnp Более современная команда. Быстрее и больше информации route Вывод таблицы маршрутизации для текущего хоста arp Просмотр содержимого ARP таблицы cat /etc/resolv.conf Просмотр используемых хостом в данный момент DNS-серверов tcpdump -i eth1 'port 80' Регистрация и отслеживание всего входящего трафика на порту 80 nmap ip Сетевое обнаружение заданного IP. Показывает: работает ли хост, открытые порты Команды поиска\nКоманда Описание locate keyword Поиск файла или каталога в кэше. Бысрее, чем find, но не всегда актуален find keyword Поиск файла или каталога в системе. Медленнее, чем locate, но всегда актуален find /home -name *.ext Поиск всех файлов с расширением .ext в домашней директории и её подкаталогах find / -type f ! -perm xxx Поиск файлов в корневой директории, у которых разрешение не соответствует ххх find / -perm /u=r Показать все файлы, доступные только для чтения grep keyword filename Поиск указанного ключевого слова в указанном файле grep keyword * Поиск указанного ключевого слова во всех файлах, в текущем каталоге grep -i keyword * Игнорировать регистр при поиске grep -r keyword * Выполнить рекурсивный поиск, т.е. включить в поиск подкаталоги grep -x 'what to match' * Вывести все строки, где найдено совпадение grep -c keyword * Подсчитать количество совпадений Команды разрешений\nКоманда Описание chmod xxx filename Назначить указанные разрешения файлу chmod –R xxx directory Назначить указанные разрешения каталогу и всем его подкаталогам chmod –x filename Удалить разрешения на выполнение файла chown username filename Изменить владельца указанного файла chown username:groupname filename Измененить владельца и группу владельца файла chown username:groupname filename1 filename2 Измененить владельца и группу владельца нескольких файлов chown --from=bob alice filename Изменить владельца файла, если файл принадлежит пользователю bob chown -h usergroup symbolic_link Принудительно изменить владельца и группу символической ссылки Команды хранения\nКоманда Описание df –h Показать используемое и свободное пространство файловых систем mount Монтировать раздел с данными или устройство unmount Размонтировать раздел с данными или устройство du -h /home/directory_name Показать размер каталога в удобном формате (human) du -sh /home/directory_name Показать общий размер директории du -ah --exclude='*.xxx' /home/directory_name Показать использование диска всеми файлами в каталоге, за исключением du -ha --time /home/directory_name Вывод файлов с датой и временем последнего изменения fdisk -l Показать размер диска с информацией о всех разделах sudo du -x / | sort -nr | head -20 Вывести 20 самых больших по размеру директорий Команды для управления пользователями\nКоманда Описание adduser username Создать нового пользователя userdel -r 'username' Удалить пользователя passwd -l 'username' Изменить пароль пользователя whoami Посмотреть текущего пользователя usermod -c 'Этот пользователь админ' username Добавление комментария к учетной записи пользователя cat /etc/passwd Вывести список всех пользователей с информацией об ID и оболочке usermod -d /home/test username Изменить домашний каталог пользователя sudo deluser username group_name Удалить пользователя из группы usermod -a -G group_name username Добавить пользователя в группу groupadd group_name Создать новую группу groupdel group_name Удалить группу id Отображение ID пользователя, его группы и групп, в которых он состоит Команды для работы с архивами\nКоманда Описание tar -cvf archive.tar filename.txt Сжать файл в архив tar tar -tvf archive.tar Отображение содержимого tar-архива tar -xvf archive.tar filename.txt Извлечь один файл из архива tar zip archive.zip file1.txt file2.html file3.jpg Создать zip-архив с использованием нескольких файлов zip -u archive.zip filename.txt Добавить файл в уже заархивированный файл zip -d archive.zip filename.txt Удалить файл из zip-архива unzip archive.zip Распаковать zip-архив unzip archive.zip -d /directory_name Распаковать zip-архив в определенный каталог tar xf archive Извлечь архив любого типа gzip filename Сжать файл и добавить к нему расширение .gz. Это удалит исходный файл gzip -c filename \u0026gt; archive.gz Создать новый сжатый файл .gz, при этом сохранив исходный Расширенные команды\nКоманда Описание grep -ir \u0026quot;слово\u0026quot; /etc/random_d Найти слово во всех файлах директории (Рекурсивно) ps –ef | grep имя процесса Проверка запущенного процесса netstat –pnltu | grep port Проверка открыт ли порт ss -tulnp Современная команда. Быстрее и больше информации history | grep keyword Поиск в истории команд ssh-keygen Генерация SSH ключей scp file user@ip:/home/location Копирование файлов на другой хост find / -type f -perm 777 -print -exec chmod xxx {} \\; Поиск всех файлов с правами доступа 777 и измените их на xxx sed -i 's/заменитьэто/наэто/g' file.txt Автозамена слова в файле file.txt sed -i -r 's/draft: true/draft: false/' /opt/* Замена всех вхождений в файлах директории /opt hostname -I | awk '{print $1}' Получить все IP адреса сетевого интерфейса хоста и вывести первый tcpdump -nnvvS src SRC_IP and dst port xxxx Анализ сетевого трафика, исходящего с определенного IP и идущий к определенному порту traceroute domain_name -q 5 Отправка 5-ти пакетов на домен. Диагностика сети ip r IP-адрес шлюза по умолчанию ","date":"2025-07-18T11:40:36+05:00","image":"https://itzm.tech/p/linux_commands/linux_hu3928115257067288558.jpg","permalink":"https://itzm.tech/p/linux_commands/","title":"100+ Полезных команд Linux"},{"content":" Через плагин Index State Management (ISM) в Opensearch Dashboards настраиваются сроки хранения и ротации логов.\nПример политики хранения логов 3 года Ключевые этапы жизненного цикла:\nГорячая фаза (Hot, 7 дней):\nИндексы активно индексируются Автоматический rollover при достижении 50 ГБ Оптимальная производительность (SSD/NVMe) Тёплая фаза (Warm, 30 дней):\nУменьшение реплик (number_of_replicas: 1) Оптимизация сегментов (force_merge) Хранение на HDD/магнитных дисках Холодная фаза (Cold, до 3 лет):\nДанные доступны только для чтения Возможность использовать объектные хранилища (S3, MinIO) Минимальная стоимость хранения Удаление (через 1095 дней):\nПолная очистка старых данных В json ISM добавить код и заменить своим Index\u0026rsquo;ом к которому применять правило\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 PUT /_plugins/_ism/policy/delete_info_1095d { \u0026#34;policy\u0026#34;: { \u0026#34;policy_id\u0026#34;: \u0026#34;delete_info_1095d\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Hot - Warm - Cold - Delete (3 года)\u0026#34;, \u0026#34;default_state\u0026#34;: \u0026#34;hot\u0026#34;, \u0026#34;states\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;hot\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;rollover\u0026#34;: { \u0026#34;min_size\u0026#34;: \u0026#34;50gb\u0026#34;, \u0026#34;min_primary_shard_size\u0026#34;: \u0026#34;10gb\u0026#34; } } ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;warm\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;7d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;warm\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;replica_count\u0026#34;: { \u0026#34;number_of_replicas\u0026#34;: 1 } }, { \u0026#34;force_merge\u0026#34;: { \u0026#34;max_num_segments\u0026#34;: 1 } }, { \u0026#34;read_only\u0026#34;: {} } ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;cold\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;30d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;cold\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1h\u0026#34; }, \u0026#34;cold_migration\u0026#34;: {} } ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;1095d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;delete\u0026#34;: {} } ] } ], \u0026#34;ism_template\u0026#34;: { \u0026#34;index_patterns\u0026#34;: [\u0026#34;info-logs-*\u0026#34;], \u0026#34;priority\u0026#34;: 80 } } } Пример политики хранения логов 1 день Отладочные логи (debug). Срок хранения - 1 день.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 { \u0026#34;policy\u0026#34;: { \u0026#34;policy_id\u0026#34;: \u0026#34;delete_debug_1d\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Delete debug logs after 1 day\u0026#34;, \u0026#34;default_state\u0026#34;: \u0026#34;hot\u0026#34;, \u0026#34;states\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;hot\u0026#34;, \u0026#34;actions\u0026#34;: [], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;1d\u0026#34; } } ] }, { \u0026#34;name\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;delete\u0026#34;: {} } ], \u0026#34;transitions\u0026#34;: [] } ], \u0026#34;ism_template\u0026#34;: { \u0026#34;index_patterns\u0026#34;: [\u0026#34;debug-delete-1d\u0026#34;], \u0026#34;priority\u0026#34;: 100 } } } Сжатие логов перед удалением Если требуется архивирование\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026#34;name\u0026#34;: \u0026#34;cold\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;snapshot\u0026#34;: { \u0026#34;repository\u0026#34;: \u0026#34;my-s3-repository\u0026#34;, \u0026#34;snapshot\u0026#34;: \u0026#34;debug-logs-archive-{{ctx.index}}\u0026#34; } } ] Примерный конфиг Fluent-bit для Kafka 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [INPUT] Name tail Path /var/log/syslog Tag server_logs parser json [FILTER] Name rewrite_tag Match server_logs Rule log debug debug false Rule log info info false Rule log warning warning false Emitter_Name log_router [OUTPUT] Name kafka Match debug Brokers kafka:9092 Topics debug-logs [OUTPUT] Name kafka Match info Brokers kafka:9092 Topics info-logs [OUTPUT] Name kafka Match warning Brokers kafka:9092 Topics warning-logs ","date":"2025-06-26T12:23:36+05:00","image":"https://itzm.tech/p/ism_policy/opensearch_hu12268508993555664199.jpg","permalink":"https://itzm.tech/p/ism_policy/","title":"Политики хранения логов Opensearch"},{"content":"Управлением кластером ELK/Opensearch в Dev Tools Состояние кластера 1 GET /_cluster/health?pretty Получение информации о нодах 1 GET /_cat/nodes Получение списка индексов и их статуса 1 GET /_cat/indices Получение настроек кластера 1 GET /_cluster/settings Изменение настроек кластера 1 PUT /_cluster/settings Информация о состоянии дисков 1 GET /_cat/allocation?v Поиск проблемного индекса 1 GET /_cat/shards?v\u0026amp;h=index,shard,prirep,state,node,unassigned.reason\u0026amp;s=state Просмотр шардов и нод, на которых они расположены 1 GET _cat/shards?v\u0026amp;h=index,shard,prirep,state,node,unassigned.reason Информация о состоянии нод ElasticSearch (Opensearch) и нагрузки 1 GET _cat/nodes?v\u0026amp;h=name,ip,node.role,heap.percent,ram.percent,cpu,load_1m,node.role,master Принудительный запуск восстановления кластера ElasticSearch (Opensearch) 1 POST _cluster/reroute?retry_failed Состояние нод кластера ElasticSearch (Opensearch) и показать кто мастер 1 GET _cat/nodes?v\u0026amp;h=name,ip,node.role,heap.percent,ram.percent,cpu,load_1m,node.role,master Перемещение шардов с ноды на ноду вручную 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 POST /_cluster/reroute { \u0026#34;commands\u0026#34;: [ { \u0026#34;move\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;my-log-2025.01.10\u0026#34;, \u0026#34;shard\u0026#34;: 0, \u0026#34;from_node\u0026#34;: \u0026#34;elk-node-1\u0026#34;, \u0026#34;to_node\u0026#34;: \u0026#34;elk-node-2\u0026#34; } } ] } Детали проблемного шарда 1 2 3 4 5 6 7 8 9 10 11 GET /_cluster/allocation/explain { \u0026#34;index\u0026#34;: \u0026#34;my-log-shard\u0026#34;, \u0026#34;shard\u0026#34;: 0, \u0026#34;primary\u0026#34;: false } ","date":"2025-06-19T18:10:36+05:00","image":"https://itzm.tech/p/elk_command/elk_hu388865683197955202.jpg","permalink":"https://itzm.tech/p/elk_command/","title":"Команды для управления кластером ElasticSearch/Opensearch"},{"content":"Bitbucket — это сервис для хостинга кода и совместной работы. Другими словами: это Git репозиторий, со своими правами и разграничениями для команд.\nТребования:\nСистема: Ubuntu Установленный: Docker, docker-compose CPU: 4 vcpu RAM: 8 Gb HDD: На ваше усмотрение (Рекомендуется 100 GB) Шаг 1. Создаём структуру 1 sudo mkdir -p /opt/bitbucket/activate И файл:\n1 sudo nano /opt/bitbucket/docker-compose.yaml Наполняем содержимым:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 version: \u0026#39;3.7\u0026#39; services: nginx: container_name: nginx image: nginx:alpine ports: - \u0026#34;443:443\u0026#34; volumes: - ./nginx/nginx.conf:/etc/nginx/nginx.conf - ./nginx/certs:/etc/nginx/certs:ro depends_on: - bitbucket networks: - bitbucket-proxy bitbucket: container_name: bitbucket image: atlassian/bitbucket:8.16.3 restart: unless-stopped volumes: - var:/var/atlassian/application-data/bitbucket - opt:/opt/atlassian/bitbucket - ./activate:/opt/atlassian/atlassian-agent/ environment: - \u0026#39;JVM_MINIMUM_MEMORY=2048m\u0026#39; - \u0026#39;JVM_MAXIMUM_MEMORY=4096m\u0026#39; - \u0026#39;SERVER_PROXY_NAME=bitbucket.domain.local\u0026#39; - \u0026#39;SERVER_SECURE=true\u0026#39; - \u0026#39;SERVER_SCHEME=https\u0026#39; - \u0026#39;SERVER_PROXY_PORT=443\u0026#39; - \u0026#39;TZ=Europe/Moscow\u0026#39; networks: - bitbucket-proxy db: image: postgres:15-alpine environment: POSTGRES_USER: bitbucket POSTGRES_PASSWORD: ChangeME POSTGRES_DB: bitbucketdb PGDATA: /data/postgres volumes: - ./postgres:/data/postgres ports: - \u0026#34;5432:5432\u0026#34; restart: unless-stopped networks: - bitbucket-proxy networks: bitbucket-proxy: volumes: var: external: false opt: external: false Сохраняем, создаём директорию для nginx\n1 mkdir -p /opt/bitbucket/nginx/certs И файл конфигурации:\n1 sudo nano /opt/bitbucket/nginx/nginx.conf Наполняем:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 events {} http { server { listen 443 ssl; server_name bitbucket.domain.local; #Меняйте на своё имя ssl_certificate /etc/nginx/certs/selfsigned.crt; ssl_certificate_key /etc/nginx/certs/selfsigned.key; location / { proxy_pass http://bitbucket:7990; proxy_http_version 1.1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port 443; proxy_redirect off; } } } Далее создаём самоподписанные сертификаты:\nПереходим в директорию:\n1 cd /opt/bitbucket/nginx Выполняем команду:\n1 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout certs/selfsigned.key -out certs/selfsigned.crt -subj \u0026#34;/C=RU/ST=Moscow/L=My/O=ITZM/OU=TECH/CN=bitbucket.domain.local\u0026#34; После переходим в директорию с docker-compose.yaml:\n1 cd /opt/bitbucket/ И запускаем сервис:\n1 docker-compose up -d Шаг 2. Настройка в веб-интерфейсе Выбираем язык. Русского Bitbucket нет, только English.\nТакже здесь выбираем Database - External\nВводим настройки для подключения, которым мы указали в docker-compose файле. Затем нажимаем Test/Next.\nШаг 3. Активация Итак, мы подходим к моменту активации.\nПроцесс активации я подробно описал в Telegram.\nТам же отвечаю на вопросы и обновляю инструкции при необходимости\nПрисоединяйся Шаг 4. Завершаем настройку в веб-интерфейсе После активации заходим используя данные нашего локального admin\u0026rsquo;a.\nСоздаём окружение и репозиторий для хранения кода.\nДалее идёт интуитивно понятная настройка используя кнопку \u0026ldquo;Далее\u0026rdquo;. В настройках меняете адрес на https://bitbucket.domain.local\nУстановка завершена. Bitbucket готов к работе.\n","date":"2025-06-15T14:30:00+05:00","image":"https://itzm.tech/p/bitbucket-docker/bitbucket_hu5786175163772573760.png","permalink":"https://itzm.tech/p/bitbucket-docker/","title":"Установка Bitbucket на Ubuntu и Docker — пошаговая инструкция"},{"content":"В Java-приложениях для обеспечения безопасности данных и установления защищённых соединений (например, HTTPS) используется специальное хранилище — keystore. Оно предназначено для хранения секретной информации, такой как закрытые ключи и сертификаты.\nСуществуют различные форматы хранилищ, поддерживаемые Java:\nJKS (Java KeyStore) — традиционный формат, использующийся по умолчанию. Представляет собой файл с расширением .jks и применяется чаще всего.\nJCEKS — расширенная версия JKS, в которой реализовано более надёжное шифрование (Triple DES). При необходимости можно преобразовать JKS-хранилище в JCEKS с помощью команды утилиты keytool.\nPKCS12 — формат, ориентированный на перенос и хранение пользовательских закрытых ключей и сертификатов. Часто используется для взаимодействия с внешними системами.\nКаждая запись в keystore идентифицируется с помощью уникального имени — alias. В целях надёжности рекомендуется избегать alias\u0026rsquo;ов, отличающихся только регистром символов.\nБезопасность данных в хранилище обеспечивается двойным способом: каждое ключевое значение может быть защищено индивидуальным паролем, а всё хранилище — отдельным общим паролем.\nВся информация в keystore делится на два типа:\nКлючевые записи — пары закрытого и открытого ключей (private/public).\nДоверенные сертификаты — сертификаты центров сертификации и других доверенных источников.\nДля управления содержимым keystore Java предоставляет встроенную утилиту keytool, находящуюся в каталоге bin установки JDK. С её помощью можно создавать, просматривать, изменять и удалять ключи и сертификаты.\nКак правило создаются keystore и truststore. Keystore - содержит личный ключ + сертификат (цепочку сертификатов) сервера, а truststore сертификат ЦА (root-ca).\nСоздание хранилища и добавление самоподписанного сертификата Генерация самоподписанного сертификата и запись его в хранилище:\n1 2 3 4 5 6 7 8 9 keytool -genkeypair \\ -alias mykey \\ -keyalg RSA \\ -keysize 2048 \\ -validity 365 \\ -keystore keystore.jks \\ -storepass your_password \\ -keypass your_password \\ -dname \u0026#34;CN=myhost, OU=Company, O=MyCompany, L=City, ST=MyState, C=RU\u0026#34; -alias — имя ключа.\n-keystore — файл keystore\u0026rsquo;а (если не существует — будет создан).\n-storepass, -keypass — пароли хранилища и ключа.\n-validity — срок действия (в днях).\n-dname — информация о владельце (DN).\nКонвертация JKS хранилища в PKCS12: Более современный формат\n1 2 3 4 keytool -importkeystore \\ -srckeystore ./mycerts/keystore.jks \\ -destkeystore ./mycerts/keystore.p12 \\ -deststoretype PKCS12 Добавление сертификата ЦА в truststore: 1 2 3 4 5 keytool -importcert \\ -alias rootca \\ -file root-ca.cer \\ -keystore truststore.jks \\ -storepass your_password Экспорт сертификата ЦА из хранилища: 1 2 3 4 5 keytool -exportcert \\ -alias rootca \\ -keystore truststore.jks \\ -file root-ca.cer \\ -storepass your_password Просмотр содержимого сертификата: 1 keytool -printcert -file myserver.cer Генерация SSL сертификата для JAVA в проде (ЦА): Создать cnf Конфиг файл для выпуска CSR\n1 sudo nano cert.cnf C cодержимым:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [req] default_bits = 2048 prompt = no default_md = sha256 req_extensions = req_extensions distinguished_name = dn [dn] C = RU ST = Moscow L = Moscow O = Company OU = IT CN = myserver.domain.local [req_extensions] subjectAltName = @alter_name extendedKeyUsage = clientAuth basicConstraints = CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment [alter_name] DNS.1 = myserver.domain.local IP.1 = 192.168.1.100 Выпустить приватный ключ Он же будет использоваться для сертификата:\n1 openssl genrsa -out certkey.key 2048 Выпустить CSR 1 openssl req -new -key certkey.key -out cert.csr -config cert.cnf Выпускаем сертификат с ЦА\nПолученный сертификат можно использовать вместе с выпущенным ранее ключом\nДля авторизации сервисов в Java приложении нужно использовать корневой сертификат + выпущенный сертификат и ключ.\nЕсли при выпуске приватного ключа был указан пароль, также нужно будет указать пароль.\nВыпустить pkcs12 password - указать новый пароль, который будет использоваться в дальнейшем:\n1 2 3 4 5 6 7 8 openssl pkcs12 -export \\ -in cert.crt \\ -inkey certkey.key \\ -out cert.p12 \\ -name cert \\ -CAfile ca.crt \\ -caname root \\ -passout pass:password Выпустить keystore 1 2 3 4 5 6 7 8 keytool -importkeystore \\ -deststorepass password \\ -destkeypass password \\ -destkeystore cert.keystore.jks \\ -srckeystore cert.p12 \\ -srcstoretype PKCS12 \\ -srcstorepass password \\ -alias cert Выпустить truststore 1 2 3 4 5 6 keytool -keystore cert.truststore.jks \\ -alias CARoot \\ -importcert \\ -file ca.crt \\ -storepass password \\ -noprompt ","date":"2025-06-05T10:10:36+05:00","image":"https://itzm.tech/p/cert_store/java_hu5578835430448783096.jpg","permalink":"https://itzm.tech/p/cert_store/","title":"Генерация сертификатов и хранилища ключей Java"},{"content":"Введение Promtail + Loki + Grafana — это связка инструментов для сбора, обработки и визуализации логов. Эти 3 важных компонента работают вместе.\nGrafana – мощный инструмент для визуализации данных. Он позволяет строить графики, панели мониторинга и работать с различными источниками данных, включая Loki. Loki – система агрегации и хранения логов, разработанная Grafana Labs. Loki похож на Prometheus, но для логов. Он индексирует не весь текст логов, а только метаданные (лейблы), что делает его быстрым и эффективным. Promtail – агент для сбора логов. Он читает логи из файлов, обрабатывает их (например, добавляет метки) и отправляет в Loki. Приложение (Пишет логи в файл) -\u0026gt; Promtail (Читает файлы, добавляет метки) -\u0026gt; Loki (Хранит логи) \u0026lt;- Grafana (Визуализирует полученные данные)\nВ Telegram я сделал более глубокий разбор функционала и архитектуры Loki, который поможет тебе хорошо разобраться как же он работает на самом деле.\nЕсли интересно — Присоединяйся Основная цель данной заметки: Сделать общее хранилище и визуализацию логов, с других серверов.\nАдрес моего сервера логирования 192.168.10.131\nШаг 1. Установка docker, docker-compose 1 2 3 4 5 6 7 8 9 sudo apt update sudo apt install -y ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo tee /etc/apt/keyrings/docker.asc \u0026gt; /dev/null sudo chmod a+r /etc/apt/keyrings/docker.asc echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] \\ https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt update sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin Проверяем установку\n1 2 docker --version docker compose version Добавляем пользователя в группу docker\n1 2 sudo usermod -aG docker $USER newgrp docker Шаг 2. Подготавливаем файлы конфига Создаём директории:\n1 2 sudo mkdir -p /opt/logging/data_loki sudo mkdir /opt/logging/promtail Конфигурация содержит 3 сервиса: Loki, Promtail и Grafana Создаём файл compose:\n1 sudo nano /opt/logging/docker-compose.yaml Наполняем:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 services: loki: container_name: loki image: grafana/loki:3.4.2 command: \u0026#34;-config.file=/etc/loki/config.yaml\u0026#34; ports: - \u0026#34;3100:3100\u0026#34; volumes: - ./data_loki/config.yaml:/etc/loki/config.yaml:ro restart: unless-stopped promtail: image: grafana/promtail:3.4.2 command: \u0026#34;-config.file=/mnt/config/config.yaml\u0026#34; volumes: - ./promtail/config.yaml:/mnt/config/config.yaml:ro - /var/run/docker.sock:/var/run/docker.sock restart: unless-stopped grafana: image: grafana/grafana-oss:11.5.2 container_name: grafana ports: - \u0026#34;3000:3000\u0026#34; restart: unless-stopped Собирать логи с контейнеров можно двумя способами:\nПервый - это установить loki docker plugin для доставки логов до Loki. Второй - это прописывать на устройствах сборку логов через docker сокет, как в нашем случае. Создаём файл конфига promtail:\n1 sudo nano /opt/promtail/config.yaml Наполняем:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://192.168.10.131:3100/loki/api/v1/push # Изменяйте на IP своего сервера с Promtail scrape_configs: # Сбор логов с Docker контейнеров - job_name: docker-logs docker_sd_configs: - host: unix:///var/run/docker.sock relabel_configs: - source_labels: [__meta_docker_container_name] target_label: container - source_labels: [__meta_docker_container_log_stream] target_label: stream # Сбор логов из файлов (например, системные логи) - job_name: system-logs static_configs: - targets: - localhost labels: job: varlogs __path__: /var/log/*.log Локи может работать как одиночный узел (Single) с локальным хранилищем (для небольших проектов), или в микросервисном (кластерном) режиме, где все его компоненты (Distributor, Ingester, Querier, Compactor, Ruler, Query Frontend) работают как отдельные сервисы, а данные сохраняются в S3, MinIO.\n1 sudo nano opt/data_loki/config.yaml Далее создаём конфиг loki.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 auth_enabled: false server: http_listen_port: 3100 grpc_listen_port: 9096 common: instance_addr: 127.0.0.1 path_prefix: /loki storage: filesystem: chunks_directory: /loki/chunks rules_directory: /loki/rules replication_factor: 1 ring: kvstore: store: inmemory schema_config: configs: - from: 2020-10-24 store: tsdb object_store: filesystem schema: v13 index: prefix: index_ period: 24h ruler: alertmanager_url: http://localhost:9093 Описание конфигурации:\nПараметр Описание auth_enabled: false Выключает авторизацию instance_addr: 127.0.0.1 Адрес на котором работает Loki http_listen_port Порт для прослушивания локи path_prefix: /loki Базовый путь, где Loki будет хранить свои файлы и данные storage.filesystem Loki хранит логи на локальной файловой системе replication_factor: 1 Количество реплик в системе ring.kvstore.store: inmemory Loki хранит метаданные в памяти Шаг 3. Запускаем 1 sudo docker-compose up -d Открываем графану по IP-адресу сервера, порт 3000\nПо умолчанию admin:admin Переходим на вкладку:\nConnections -\u0026gt; Add new connection -\u0026gt; Loki -\u0026gt; URL: http://192.168.10.131:3100 -\u0026gt; Save and Test\nЕсли все сделано правильно подключение активируется и графана начнёт отображать логи во вкладке Explore -\u0026gt; Logs\nШаг 4. Подключаем другой docker-хост Заходим на сервер, который будет отправлять логи. Создаём директории:\n1 2 3 sudo mkdir /opt/promtail cd /opt/promtail/ sudo nano docker-compose.yaml Наполняем:\n1 2 3 4 5 6 7 8 services: promtail: image: grafana/promtail:3.4.2 command: \u0026#34;-config.file=/mnt/config/config.yaml\u0026#34; volumes: - ./config/config.yaml:/mnt/config/config.yaml:ro - /var/run/docker.sock:/var/run/docker.sock restart: unless-stopped И создаём конфигурацию:\n1 2 sudo mkdir -p ./config/ sudo nano ./config/config.yaml Подключаем сервер на отправку логов:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 server: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://192.168.10.131:3100/loki/api/v1/push # Изменяйте на IP своего сервера с Promtail scrape_configs: - job_name: docker-logs docker_sd_configs: - host: unix:///var/run/docker.sock relabel_configs: - source_labels: [__meta_docker_container_name] target_label: container - source_labels: [__meta_docker_container_log_stream] target_label: stream Запускаем:\n1 docker-compose up -d Для примера на данном сервере запущу контейнер с nginx\n1 docker run --name nginx -p 8080:80 -d nginx Идем в графану Explore - Logs и смотрю логи\nНа этом базовая настройка завершена.\n","date":"2025-05-03T11:15:32+05:00","image":"https://itzm.tech/p/loki-in-docker/loki_hu4654732748976078740.jpg","permalink":"https://itzm.tech/p/loki-in-docker/","title":"Promtail, Loki, Grafana в docker"},{"content":"Требования Развернутый K8S, Minikube или K3S\nЕще одна причина поставить кубернетес, это поиграть в старый-добрый Марио. Сделать это проще простого:\nЗаходим на master-ноду. Клонируем репозиторий:\n1 git clone https://github.com/ZM56/super-mario-k8s.git Создаём namespace:\n1 kubectl create ns game Далее переходим в директорию и деплоим:\n1 2 cd super-mario-k8s kubectl apply -f . Проверяем, что под поднялся:\n1 kubectl get pod -n game Переходим в браузер. Вводим IP:31111\nУправление расписано слева сверху. Наслаждаемся :)\nУдаление:\n1 kubectl delete ns game ","date":"2025-04-27T10:37:00+05:00","image":"https://itzm.tech/p/mario_k8s/supermario_hu15720909271051765619.jpg","permalink":"https://itzm.tech/p/mario_k8s/","title":"Super Mario в Kubernetes за 5 минут"},{"content":"Появилась необходимость на скорую руку поднять FTPS для обмена с внешними сервисами.\nЕсть два режима пользователей в контексте vsftpd: локальные (системные) и виртуальные. В данном примере будут созданы виртуальные пользователи. В этом режиме пользователи не имеют системных учетных записей, а управляются через базу данных или файл.\nКак это работает:\nКлиент подключается к серверу, отправляет логин и пароль Сервер проверяет учетные данные в базе данных виртуальных пользователей и представляет доступ к определенным директориям. Решение более безопасное, так как виртуальные пользователи не имеют доступа к системе. Плюсом получаем упрощенное управление пользователями.\nШаг 1. Установка Обновляем списки портов в репозиториях:\n1 sudo apt update Устанавливаем vsFTPd:\n1 sudo apt install vsftpd -y Устанавливаем утилиту для создания базы данных пользователей:\n1 sudo apt install db-util -y Основной конфигурационный файл находится здесь /etc/vsftpd.conf\nДелаем копию оригинального конфига:\n1 sudo cp /etc/vsftpd.conf /etc/vsftpd.conf.bak Шаг 2. Конфигурация сервера Открываем новый файл на редактирование:\n1 sudo nano /etc/vsftpd.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 ### Основные настройки ## Сервер работает в standalone-режиме. Сам прослушивает и определяет входящие соединения. listen=YES # Анонимный доступ запрещен anonymous_enable=NO # Разрешен доступ локальным пользователям системы local_enable=YES # Включена поддержка виртуальных пользователей guest_enable=YES # Виртуальные пользователи работают от имени системного пользователя vsftpd guest_username=vsftpd # Виртуальные пользователи имеют те же права, что и локальные пользователи virtual_use_local_privs=YES # Используется PAM-модуль vsftpd.virtual для аутентификации виртуальных пользователей pam_service_name=vsftpd.virtual # Каждый пользователь имеет свою домашнюю директорию в /home/vsftpd/ user_sub_token=$USER local_root=/home/vsftpd/$USER ## Настройки chroot # Чтобы пользователи не могли перемещаться по каталогам сервера и просматривать файлы ограничиваем их домашними директориями chroot_local_user=YES allow_writeable_chroot=YES ## Логирование # Скрывает реальные UID/GID пользователей в логах hide_ids=YES # Включает логирование передачи файлов xferlog_enable=YES # Включает подробное логирование FTP-протокола log_ftp_protocol=YES # Указывает путь к файлу логов передачи файлов xferlog_file=/var/log/vsftpd.log vsftpd_log_file=/var/log/vsftpd.log # Более подробный вывод логов xferlog_std_format=NO ## Настройки портов # Разрешает активный режим FTP на 20 порту port_enable=YES connect_from_port_20=YES ftp_data_port=20 # Минимальный и максимальный порты, для пассивного режима pasv_enable=YES pasv_max_port=35109 pasv_min_port=35100 ## Права доступа для создаваемых файлов file_open_mode=0666 local_umask=077 ## Настройки SSL/TLS ssl_enable=YES # Запрещает анонимным пользователям использовать SSL/TLS allow_anon_ssl=NO force_local_data_ssl=YES force_local_logins_ssl=YES require_ssl_reuse=NO strict_ssl_read_eof=NO ssl_tlsv1=NO ssl_tlsv11=NO ssl_tlsv12=YES ssl_sslv2=NO ssl_sslv3=NO rsa_cert_file=/etc/ssl/certs/vsftpd.pem rsa_private_key_file=/etc/ssl/private/vsftpd.key Все инструкции, для конфигурации подробно описаны здесь: https://www.opennet.ru/base/net/vsftpd_overview.txt.html\nШаг 3. SSL-Сертификат Для теста сгенирируем самоподписанный сертификат и ключ:\n1 2 3 4 openssl req -new -x509 -days 365 -nodes \\ -out /etc/ssl/certs/vsftpd.pem \\ -keyout /etc/ssl/private/vsftpd.key \\ -subj \u0026#34;/C=RU/ST=Moscow/L=Moscow/O=MyOrganization/OU=IT/CN=vsftpd.server\u0026#34; Шаг 4. Настройка виртуальных пользователей Создайте текстовый файл с логинами и паролями. Например, создаём файл virtual_users.txt\n1 sudo nano /etc/vsftpd/virtual_users.txt Добавляем логины и пароли в формате:\n1 2 3 4 user1 password1 user2 password2 Создаем базу данных из этого файла:\n1 sudo db_load -T -t hash -f /etc/vsftpd/virtual_users.txt /etc/vsftpd/virtual_users.db Установите права доступа к файлу базы данных:\n1 sudo chmod 600 /etc/vsftpd/virtual_users.db Настройте PAM для использования этой базы данных. Создайте файл /etc/pam.d/vsftpd.virtual:\n1 sudo nano /etc/pam.d/vsftpd.virtual Добавьте следующие строки:\n1 2 auth required pam_userdb.so db=/etc/vsftpd/virtual_users account required pam_userdb.so db=/etc/vsftpd/virtual_users Создаем пользователя vsftpd:\n1 sudo adduser --home /home/vsftpd --no-create-home --shell /bin/false vsftpd Создаём директории для виртуальных пользователей:\n1 2 sudo mkdir -p /home/vsftpd/user1 sudo mkdir -p /home/vsftpd/user2 Выдаём права на директории vsftpd:\n1 2 sudo chown vsftpd:vsftpd /home/vsftpd/user1 sudo chown vsftpd:vsftpd /home/vsftpd/user2 Перезапускаем сервис. Проверяем работоспособность\n1 sudo systemctl restart vsftpd Подключить можно через WinSCP и положить файл.\n","date":"2025-04-04T00:10:05+05:00","image":"https://itzm.tech/p/vsftpd/vsftpd_hu4054488657330172795.jpg","permalink":"https://itzm.tech/p/vsftpd/","title":"Установка vsFTPd на Ubuntu 22.04"},{"content":"В данном примере производится оффлайн установка плагинов с магазина Atlassian внутрь докер контейнера, в котором крутится Confluence или Jira. Даже если у вас не используется докер общая логика будет понятна.\nУстановка в Confluence Шаг 1. Загрузка плагина с сайта На примере плагина PlantUML\nВходим через google аккаунт, или регистрируемся на https://marketplace.atlassian.com/\nИщем в поиске плагин PlantUML\nПереходим в карточку приложения\nВ примере установлена версия Confluence Data Center, поэтому выбираем Data Center и перейти на вкладку Installation\nДалее \u0026ldquo;version history page\u0026rdquo;\nData Center - Развернуть вкладку - Download installer\nЗагрузится .jar файл, который мы копируем (через WinSCP) на сервер Confluence в домашнюю директорию пользователя admin\nШаг 2. Установка Копируем плагин из домашней директории в контейнер\n1 sudo cp /home/admin/pluginname.jar /data/docker/volumes/confluence_opt/_data/confluence/WEB-INF/atlassian-bundled-plugins/ Изменяем права на файл\n1 sudo chown 2002:root /data/docker/volumes/confluence_opt/_data/confluence/WEB-INF/atlassian-bundled-plugins/pluginname.jar И изменяем разрешения на 550\n1 chmod 550 /data/docker/volumes/confluence_opt/_data/confluence/WEB-INF/atlassian-bundled-plugins/pluginname.jar Перезапускаем контейнер\n1 docker restart confluence Проверяем работоспособность плагина в веб-интерфейсе.\nУправления приложениями -\u0026gt; Системные (или приобретенные в Atlassian)\nРабота с .obr файлами Если файл плагина, который был скачан с маркета Atlassian имеет расширение .obr, тогда:\nПереименовываем файл в zip архив\n1 sudo mv pluginname.obr pluginname.zip Разархивируем\n1 sudo unzip pluginname.zip -d /home/admin Далее проводим процедуру установки из шага 2\nДля платных плагинов Confluence Процесс активации плагинов Confluence я подробно описал в своём Telegram.\nТам же отвечаю на вопросы и обновляю инструкции при необходимости.\nПрисоединяйся Установка в Jira Для Jira установка аналогичная, заисключением путей:\nВыполняем шаг 1 из инструкции выше\nДалее устанавливаем. Копируем плагин из домашней директории в контейнер\n1 sudo cp /home/admin/pluginname.jar /data/docker/volumes/jira_var/_data/plugins/installed-plugins Имзеняем права на файл\n1 sudo chown 2002:root /data/docker/volumes/jira_var/_data/plugins/installed-plugins/pluginname.jar И изменяем разрешения на 550\n1 sudo chmod 550 /data/docker/volumes/jira_var/_data/plugins/installed-plugins/pluginname.jar Перезапускаем контейнер\n1 docker restart jira Для платных плагинов Jira Процесс активации плагинов в Jira я подробно описал в своём Telegram.\nТам же отвечаю на вопросы и делюсь знаниями.\nПодписывайся ","date":"2025-02-21T21:11:45+05:00","image":"https://itzm.tech/p/plugins_atlassian/market_hu6493302584727464933.jpg","permalink":"https://itzm.tech/p/plugins_atlassian/","title":"Offline установка плагинов в Confluence, Jira"},{"content":" Для использования защищенного https подключения Linux-серверами установим корневой сертификат. Сгенерировать сертификат, при наличии локального Центра сертификации можно двумя способами.\n1 способ генерации сертификата. Через Веб-интерфейс. Создаём запрос серта на центре сертификации. В адресной строке браузера вводим имя или ip сервера сертификации.\nhttp://\u0026lt;ip_адрес_сервера_сертификации\u0026gt;/certsrv\nЗапрос сертификата Выбираем: Base64 Скачиваем:\nУстановка в Ubuntu: Устанавливаем пакет для работы с сертификатами, если его нет:\n1 sudo apt-get install -y ca-certificates Копируем (Например, через WinSCP) сертификат сделанный в первом шаге на Linux-сервер в директорию:\n/usr/local/share/ca-certificates/\nПереходим в директорию на сервере:\n1 sudo cd /usr/local/share/ca-certificates/ Переделываем расширение в .crt, где ca_cert.cer имя вашего сертификата сгенерированного первым способом\n1 sudo openssl x509 -inform PEM -in ca_cert.cer -out ca_cert.crt Устанавливаем сертификат в хранилище:\n1 sudo update-ca-certificates После успешной установки появится сообщение, что сертификаты обновлены:\n1 2 3 Updating certificates in /etc/ssl/certs… 1 added, 0 removed; done. Running hooks in /etc/ca-certificates/update.d 2 способ генерации сертификата. CMD. Зайти на сервер корневого центра сертификации. Под администратором открыть консоль cmd\nВыполнить команду:\n1 certutil -ca.cert ca_name.cer Полученный текст сертификата скопировать в файл на Linux-сервере:\n1 sudo nano /usr/local/share/ca-certificates/ca_cert.crt Получится текст такого формата:\n1 2 3 4 5 ---BEGIN CERTIFICATE----- MIIH/TCCBeWgAwIBAgIQaBYE3/M08XHYCnNVmcFBcjANBgkqhkiG9w0BAQsFADBy ... F10YlqcOmeX1uFmKbdi/XorGlkCoMF3TDx8rmp9DBiB/ -----END CERTIFICATE----- Затем выполнить команду:\n1 sudo update-ca-certificates Установка в CentOS Устанавливаем пакет для работы с сертификатами, если его нет:\n1 yum install ca-certificates Копируем (Например, через WinSCP) сертификат сделанный в первом способе на Linux-сервер в директорию:\n1 /etc/pki/ca-trust/source/anchors/ Переходим в директорию на сервере:\n1 cd /etc/pki/ca-trust/source/anchors/ Переделываем расширение в .crt, где ca_cert.cer имя вашего сертификата сгенерированного первым способом\n1 sudo openssl x509 -inform PEM -in ca_cert.cer -out ca_cert.crt Устанавливаем в хранилище:\n1 sudo update-ca-trust extract Проверить установленный сертификат:\n1 trust list | grep Имя_Сертификата_ЦА ! Важно\nВ Linux сертификаты автоматически не устанавливаются в браузеры, и браузеры в десктопном Linux по прежнему будут выдавать ошибку. В данном случае установка сертов используется для curl и python библиотеки request, поэтому этой установки достаточно. Перекодировка сертификатов 1 2 3 4 5 openssl rsa -in mycert.pem -out mycert.key openssl x509 -inform DER –in mycert.cer -out mycert.crt openssl x509 -inform PEM -in mycert.cer -out mycert.crt openssl pkcs7 -print_certs -in mycert.p7b -out mycert.crt openssl pkcs7 -print_certs -in mycert.p7b -out mycert.pem ","date":"2025-02-04T20:05:35+05:00","image":"https://itzm.tech/p/linux_cert/certs_hu12596763269768902036.jpg","permalink":"https://itzm.tech/p/linux_cert/","title":"Установка корневого сертификата Linux"},{"content":"Kubespray - является набором Ansible скриптов, для установки Kubernetes.\nКластер будет состоять из 2-х Master node, 2-х Worker node.\nУстановку кластера будем производить с Master-1, поэтому на него установим компоненты для ansible. Пользователь на всех машинах - admin с правами sudo.\nТребования по ресурсам:\nMaster:\nCPU: 2 RAM: 4 Gb SSD: 50 Gb OS: Ubuntu 22.04 Worker:\nCPU: 4 RAM: 8 Gb SSD: 100 Gb OS: Ubuntu 22.04 IP-addresses:\nMaster-1: 192.168.10.201\nMaster-2: 192.168.10.202\nNode-1: 192.168.10.205\nNode-2: 192.168.10.206\nЛокальный пользователь, под которым выполняются команды: admin\nШаг 1. Подготовка виртуальных серверов Предполагается установка ОС Ubuntu 22.04 на один хост, с дальнейшим клонированием хостов\nСервер: master-1 Создаем машину, с именем master-1. Первым делом отключаем подкачку:\n1 sudo swapoff -a Удаляем строку из /etc/fstab\n1 /swap.img none swap sw 0 0 Отключаем DHCP. Открываем на редактирование:\n1 nano /etc/netplan/00-installer-config.yaml Прописываем статику для master-1:\n1 2 3 4 5 6 7 8 9 10 11 12 network: ethernets: ens33: dhcp4: false addresses: - 192.168.10.201/24 routes: - to: default via: 192.168.10.2 nameservers: addresses: - 8.8.8.8 Сохраняем, закрываем, применяем:\n1 admin@master-1:~$ sudo netplan apply -f Установим авторизацию для пользователя admin (мой локальный пользователь с sudo привилегиями) без пароля, для этого. В директории /etc/sudoers.d/ создаём файл admin:\n1 admin ALL=(ALL) NOPASSWD: ALL Сервер: master-2 Клонируем master-1 Меняем имя хоста на master-2 и IP в netplan 1 sudo hostnamectl set-hostname master-2 Не забудьте изменить имя сервера в файле /etc/hosts\nУбираем DHCP. Открываем на редактирование:\n1 admin@master-1:~$ nano /etc/netplan/00-installer-config.yaml Прописываем статику для master-2:\n1 2 3 4 5 6 7 8 9 10 11 12 network: ethernets: ens33: dhcp4: false addresses: - 192.168.10.202/24 routes: - to: default via: 192.168.10.2 nameservers: addresses: - 8.8.8.8 Сохраняем, закрываем, применяем:\n1 admin@master-1:~$ sudo netplan apply -f Сервер: node-1 Клонируем master-1 Меняем имя хоста на node-1 и IP в netplan 1 sudo hostnamectl set-hostname node-1 Не забудьте изменить имя сервера в файле /etc/hosts\nУбираем DHCP. Открываем на редактирование:\n1 admin@master-1:~$ nano /etc/netplan/00-installer-config.yaml Прописываем статику для node-1:\n1 2 3 4 5 6 7 8 9 10 11 12 network: ethernets: ens33: dhcp4: false addresses: - 192.168.10.205/24 routes: - to: default via: 192.168.10.2 nameservers: addresses: - 8.8.8.8 Сохраняем, закрываем, применяем:\n1 admin@master-1:~$ sudo netplan apply -f Сервер: node-2 Клонируем master-1 Меняем имя хоста на node-2 и IP в netplan 1 sudo hostnamectl set-hostname node-2 Не забудьте изменить имя сервера в файле /etc/hosts\nУбираем DHCP. Открываем на редактирование:\n1 admin@master-1:~$ nano /etc/netplan/00-installer-config.yaml Прописываем статику для node-2:\n1 2 3 4 5 6 7 8 9 10 11 12 network: ethernets: ens33: dhcp4: false addresses: - 192.168.10.206/24 routes: - to: default via: 192.168.10.2 nameservers: addresses: - 8.8.8.8 Сохраняем, закрываем, применяем:\n1 admin@master-1:~$ sudo netplan apply -f Шаг 2. Настраиваем подключение к хостам для Kubespray На master-1 создаём SSH-ключ для подключения к другим машинам:\n1 ssh-keygen -t rsa И копируем его на все сервера:\n1 2 3 ssh-copy-id admin@192.168.10.202 ssh-copy-id admin@192.168.10.205 ssh-copy-id admin@192.168.10.206 Устаналиваем компоненты Python:\n1 2 sudo apt update sudo apt install python3-pip Устанавливаем компонент, для использования ssh паролей:\n1 sudo apt install sshpass Так как вначале мы обусловились, что установка кластера будет происходить с ноды master-1, то переходим в директорию opt и выполняем клонирование репозитория:\n1 2 3 cd /opt sudo git clone https://github.com/kubernetes-sigs/kubespray.git cd kubespray Устанавливаем зависимости из файла:\n1 pip3 install -r requirements.txt Делаем копию директории:\n1 sudo cp -rfp inventory/sample inventory/k8s_cluster И меняем разрешения, чтобы избежать ощибок доступов при установке:\n1 sudo chmod -R 777 /opt/kubespray/inventory/k8s_cluster Удаляем все из файла /k8s_cluster/inventory.ini:\nОткрываем на редактирование:\n1 sudo nano inventory/k8s_cluster/inventory.ini Опишем список наших машин в файле inventory:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [all] master-1 ansible_host=192.168.10.201 ip=192.168.10.201 master-2 ansible_host=192.168.10.202 ip=192.168.10.202 node-1 ansible_host=192.168.10.205 ip=192.168.10.205 node-2 ansible_host=192.168.10.206 ip=192.168.10.206 [kube_control_plane] master-1 master-2 [etcd] master-1 [kube_node] node-1 node-2 [calico_rr] [k8s_cluster:children] kube_control_plane kube_node calico_rr Чтобы в дальнейшем использовать Helm-charts заранее отметим этот компонент на установку для всего кластера в файле /k8s_cluster/group_vars/k8s_cluster/addons.yaml:\n1 cd /opt/kubespray/inventory/k8s_cluster/group_vars/k8s_cluster Открываем на редактирование:\n1 sudo nano addons.yaml Устанавливаем значение true:\n1 helm_enabled: true В файле group_vars/k8s_cluster/k8s-cluster.yml указываем:\n1 2 3 4 5 6 7 8 9 10 container_manager: containerd kube_network_plugin: calico kube_service_addresses: 10.233.0.0/18 kube_pods_subnet: 10.233.64.0/18 kube_proxy_mode: ipvs cluster_name: cluster.k8s.local #Имя и домен вашего кластера, меняйте на свой event_ttl_duration: \u0026#34;2h0m0s\u0026#34; auto_renew_certificates: true В файле group_vars/k8s_cluster/addons.yml включаем ingress:\n1 2 3 metrics_server_enabled: true ingress_nginx_enabled: true ingress_nginx_host_network: true Чтобы постоянно не указывать от какого пользователя мы хотим запускать команды ansible можно указать в файле ansible.cfg:\n1 cd /opt/kubespray Вставляем в блоке [ssh_connection]:\n1 remote_user=admin Доустановлю ansible на master-1:\n1 sudo apt install ansible -y Шаг 3. Запуск развертывания После всех настроек выше мы запускаем наш кластер:\n1 ansible-playbook -u admin -i inventory/k8s_cluster/inventory.ini -b cluster.yml Если ошибки по подключению к master-1, то добавляем запрос ssh паролей \u0026ldquo;-kK\u0026rdquo; в конец команды:\n1 ansible-playbook -u admin -i inventory/k8s_cluster/inventory.ini -b cluster.yml -kK Установка занимает в среднем от 30 до 60 минут\nПосле установки команды будут работать только от пользователя root. Сделаем это для текущего пользователя (admin):\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Проверяем состояние всех нод:\n1 admin@master-1:/opt/kubespray$ kubectl get nodes Затем проверяем запуск всех системных подов:\n1 admin@master-1:/opt/kubespray$ kubectl get pods -n kube-system Статус нод должен быть: Ready, если возникают ошибки смотрите логи.\nРазвертывание кластера завершено.\nДополнения\nЧтобы добавить новую ноду, без изменений для уже существующих внесите её в файл inventory/k8s_cluster/inventory.ini и запустите с параметром \u0026ndash;limit:\n1 ansible-playbook -u admin -i inventory/k8s_cluster/inventory.ini --limit=nodename -b scale.yml Чтобы удалить ноду:\n1 ansible-playbook -u admin -i inventory/k8s_cluster/inventory.ini -e node=nodename -b remove-node.yml Чтобы удалить весь кластер:\n1 ansible-playbook -u admin -i inventory/k8s_cluster/inventory.ini -b reset.yml ","date":"2025-01-12T15:57:25+05:00","image":"https://itzm.tech/p/k8s/kubespray1_hu18429403248903324649.jpg","permalink":"https://itzm.tech/p/k8s/","title":"Установка K8S кластера через Kubespray"},{"content":"Мы рассмотрим процесс установки Ansible AWX, веб-интерфейса с открытым исходным кодом для управления проектами Ansible, с помощью однонодового K3S и awx-operator, на базе Ubuntu 22.04.\nAnsible AWX — это мощный веб-интерфейс с открытым исходным кодом для управления проектами Ansible, плейбуками, списками инвентори и планированием заданий по расписанию. С Ansible AWX можно эффективно и наглядно управлять инфраструктурой, организовывать задачи и оптимизировать процессы DevOps.\nВерсия ПО Client Version v1.30.5+k3s1 Kustomize Version v5.0.4-0.20230601165947-6ce0bf390ce3 Server Version v1.30.5+k3s1 Оператор AWX 2.9.1 PostgreSQL 15 Минимальные требования:\nCPU: 2 RAM: 4 Gb SSD: 50 Gb OS: Ubuntu 22.04 Рекомендованные:\nCPU: 4 RAM: 8 Gb SSD: 50 Gb OS: Ubuntu 22.04 Для чистой установки потребуется не менее 20 Gb для /var/lib/rancher. Оба показателя будут увеличиваться в течение срока использования, а фактическое потребление во многом зависит от варианта использования, поэтому следует обращать внимание на потребление и при необходимости увеличивать объём. Директория /var/lib/rancher будет создана автоматически для K3S и связанных с ним данными, такими как образы контейнеров и оверлеи. Все действия можно производить как от пользователя с правами sudo, так и от root (sudo -s)\nШаг 1. Подготовка хоста Обновить пакеты и систему 1 sudo apt update -y \u0026amp;\u0026amp; sudo apt upgrade -y Отключаем Firewall и swap 1 2 sudo ufw disable sudo swapoff -a Открываем файл на редактирование:\n1 sudo nano /etc/fstab Удаляем строку\n1 /swap.img none swap sw 0 0 Установить git, curl, jq и необходимые пакеты 1 sudo apt install git curl jq build-essential -y Шаг 2. Установка K3S - облегченный Kubernetes Указываем параметр --write-kubeconfig-mode 644, чтобы иметь доступ к конфигу на чтение без root привилегий\n1 curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644 Кластер установится автоматически. Следующим шагом проверяем нашу установку k3s с помощью команды:\n1 kubectl get nodes Создаём пространство имен для оператора\n1 2 export NAMESPACE=awx kubectl create ns ${NAMESPACE} Шаг 3. Установка Ansible AWX Клонируем официальный репозиторий awx-operator к себе на локальную машину\n1 sudo git clone https://github.com/ansible/awx-operator.git Шаг 4. Конфигурирование и запуск Изменяем текущий контекст на тот, который мы задали в переменной (awx)\n1 sudo kubectl config set-context --current --namespace=$NAMESPACE Переходим в директорию awx\n1 cd awx-operator/ Сохраняем последнюю версию из релизов AWX Operator как переменную RELEASE TAG, а затем выполняем извлечение в ветку с помощью git\u0026rsquo;a\n1 2 3 RELEASE_TAG=`curl -s https://api.github.com/repos/ansible/awx-operator/releases/latest | grep tag_name | cut -d \u0026#39;\u0026#34;\u0026#39; -f 4` echo $RELEASE_TAG git checkout $RELEASE_TAG И выполняем развёртывание\n1 make deploy После чего можно наблюдать запуск подов в реальном времени командой\n1 kubectl get pods -w *P.S. Если хотите удалить деплой, то выполните команду 1 2 export NAMESPACE=awx make undeploy Через примерно 5 минут под с AWX запустится AWX-operator установлен. Далее устанавливаем веб-интерфейс\nСоздать файл\n1 nano kustomization.yaml 1 2 3 4 5 6 7 8 9 apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - github.com/ansible/awx-operator/config/default?ref=2.19.1 - awx-demo.yml images: - name: quay.io/ansible/awx-operator newTag: 2.19.1 namespace: awx Применяем файл\n1 kubectl apply -k . Ожидаем развёртывания\u0026hellip;\nПросматриваем на каком порту слушает сервис ansible\n1 kubectl get svc Получаем пароль от веб-интерфейса\n1 kubectl get secret awx-demo-admin-password -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 --decode ; echo И заходим по IP:Port\n1 2 Логин: admin Пароль: из команды выше Шаг 5. Устанавливаем Ingress-conroller и Load Balancer Порт генерируется случайным образом и нас это не особо устраивает. Установим ingress-controller и изменим тип сервиса\nДля этого деплоим конфиг ingress-nginx:\n1 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.7.1/deploy/static/provider/baremetal/deploy.yaml Конфигурируем файл awx-demo.yaml\n1 2 3 4 5 6 7 --- apiVersion: awx.ansible.com/v1beta1 kind: AWX metadata: name: awx-demo spec: service_type: ClusterIP # Здесь замена с NodePort на ClusterIP Удаляем старый сервис\n1 kubectl delete svc awx-demo-service И применяем новый\n1 kubectl apply -f awx-demo.yml Создаём файл ingress.yaml\n1 sudo nano ingress.yaml Добавляем содержимое:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: site-nginx-ingress annotations: nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; spec: rules: - host: awx.domain.local # адрес нашего awx сайта http: paths: - path: / pathType: Prefix backend: service: name: awx-demo-service # Адрес нашего нового сервиса с типом ClusterIP port: number: 80 # Прослушиваемый порт Применяем созданный конфиг\n1 kubectl apply -f ingress.yaml Создаём файл для Load Balancer\n1 sudo nano lb.yaml Добавляем содержимое:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Service metadata: name: ingress-nginx-controller-loadbalancer namespace: ingress-nginx spec: selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx ports: - name: http port: 80 protocol: TCP targetPort: 80 - name: https port: 443 protocol: TCP targetPort: 443 type: LoadBalancer Применяем конфиг балансировщика\n1 kubectl apply -f lb.yaml Проверяем наш ingress\n1 kubectl get ing На этом этапе имя сервера уже должно быть создано в DNS и резолвиться\nПри входе по IP вы получите ошибку 404\nТеперь заходим по имени нашего сервера в веб-консоль, и можем пользоваться. На этом установка завершена.\n","date":"2024-10-18T20:19:36+05:00","image":"https://itzm.tech/p/kubernetes/awx2_hu14760130317474580920.jpg","permalink":"https://itzm.tech/p/kubernetes/","title":"Установка Ansible AWX в K3S"},{"content":"Однообразные действия, которые я выполняю более 2-х раз я всячески пытаюсь автоматизировать. Поэтому появилась необходимость организовать процесс добавления новых серверов под управление домена на базе FreeIPA. Используя ансибл-роли мы можем оперировать только заранее известными нам серверами, но мы заранее не знаем, какой ip и hostname получит сервер. Для решения этой задачи я набросал bash скрипт, который будет запускаться раз в сутки с сервера Касперского и \u0026ldquo;затаскивать\u0026rdquo; машину в домен.\nСам скрипт здесь не представлен, но я заметил отсутствие информации о запуске скриптов с Каспера. Этот процесс я опишу здесь.\nШаг 1. Подготовка файла скрипта Скрипт должен быть написан в оболочке Linux, если вы пишите в Notepad ++, то Windows может добавить лишние пробелы (которые не видит даже редактор vim). Если всё же написано в винде, то заходим в Notepad ++ - Вид отображение символов - Отображать все символы Найти и заменить все пробелы, обозначаются как /r на пустоту Или второй вариант Правка - Формат Конца Строк - Преобразовать в Unix Если скрипт копируется с Linux машины заранее дайте ему право на выполнение: 1 sudo chmod +x add_freeipa.sh И скопируйте на сервер Касперского через WinSCP с соответствующими правами, или командой scp (Если установлен OpenSSH на Windows)\nШаг 2. Создание инсталяционного пакета Заходим на сервер Касперского - Инсталяционные пакеты - Создать инсталяционный пакет\nДля указанного исполняемого файла\nВыбираем наш скрипт, который мы скопировали на сервер Касперского\nПосле чего установится пакет, который будет размещен в общей папке сервера\nПараметры отсутствуют. Если указать их, они добавятся в конце команды по запуску скрипта на конечном сервере.\nНапример, если в поле вписать sudo -s, то выполнение будет происходит так: /bin/sh \u0026hellip; \u0026lsquo;add_freeipa.sh\u0026rsquo; sudo -s. То есть в заполнении этого поля нет необходимости, так как параметр не повлияет на скрипт.\nШаг 3. Создание задачи установки Задачу можно создать как на группу устройств (без наследования), так и на отдельные устройства, с параметрами запуска \u0026ldquo;По расписанию\u0026rdquo; и указать время запуска задания.\nНажимаем на созданном нами исталяционном пакете \u0026ldquo;Установить программу\u0026rdquo;\nВыбираем на группу и выбираем устройства, на которые будет распространяться задача\nОбязательно отметьте галкой пункт \u0026ldquo;Не устанавливать программу, если она уже установлена\u0026rdquo;\nВ случае, если в логах Вы увидите \u0026ldquo;Permission Denied\u0026rdquo;, то укажите креды для пользователя имеющего root авторизацию. У меня всё сработало без него. Наблюдаем за выполнением и читаем конечный лог задачи\nЕсли скрипт будет выдавать ошибку она будет отображена здесь, но статус задачи всё равно будет \u0026ldquo;Завершена успешно\u0026rdquo;, поэтому чтобы отлаживать скрипт используйте \u0026ldquo;Результаты выполнения задания\u0026rdquo; В моём случае задача выполнилась успешно, оповещение WARNING можно не брать во внимание.\n","date":"2024-10-09T20:20:54+05:00","image":"https://itzm.tech/p/kaspersky-bash/kaspersky_hu12769972192133205768.jpg","permalink":"https://itzm.tech/p/kaspersky-bash/","title":"Запуск bash скриптов используя Kaspersky Security Center"},{"content":"Confluence - это вики-система для внутреннего использования организациями, с целью создания единой базы знаний.\nСамый быстрый способ развертывания данной системы, это использование докер контейнеров. Установка схожа с развертыванием другого продукта Atlassian - Jira\n-\u0026gt; На сайте есть заметка\nТребования:\nСистема: Ubuntu 22.04 Установленный: Docker, docker-compose CPU: 4 vcpu RAM: 8 Gb HDD: На ваше усмотрение (Рекомендуется 300 GB, на 500+ пользователей) Шаг 1. Создаём структуру Создаём директории:\n1 sudo mkdir -p /opt/confluence/activate И файл:\n1 sudo nano /opt/confluence/docker-compose.yaml Наполняем содержимым:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 version: \u0026#39;3.7\u0026#39; services: nginx: container_name: nginx image: nginx:alpine ports: - \u0026#34;443:443\u0026#34; volumes: - ./nginx/nginx.conf:/etc/nginx/nginx.conf - ./nginx/certs:/etc/nginx/certs:ro depends_on: - confluence networks: - conf-proxy confluence: container_name: confluence image: atlassian/confluence:9.0.3 restart: unless-stopped volumes: - var:/var/atlassian/application-data/confluence - opt:/opt/atlassian/confluence - ./activate:/opt/atlassian/atlassian-agent/ environment: - \u0026#39;CONFLUENCE_DB_HOST=db\u0026#39; - \u0026#39;CONFLUENCE_DB_PORT=5432\u0026#39; - \u0026#39;JVM_MINIMUM_MEMORY=2048m\u0026#39; - \u0026#39;JVM_MAXIMUM_MEMORY=4096m\u0026#39; - \u0026#39;ATL_PROXY_NAME=confluence.domain.local\u0026#39; - \u0026#39;ATL_PROXY_PORT=443\u0026#39; - \u0026#39;ATL_TOMCAT_SCHEME=https\u0026#39; - \u0026#39;TZ=Europe/Moscow\u0026#39; networks: - conf-proxy db: image: postgres:15-alpine environment: POSTGRES_USER: confluencedb POSTGRES_PASSWORD: changeME # (Изменить на свой) POSTGRES_DB: confluencedb PGDATA: /data/postgres volumes: - ./postgres:/data/postgres ports: - \u0026#34;5432:5432\u0026#34; restart: unless-stopped networks: - conf-proxy networks: conf-proxy: volumes: var: external: false opt: external: false Сохраняем, создаём директорию для nginx\n1 mkdir -p /opt/confluence/nginx/certs И конфигурацию:\n1 sudo nano /opt/confluence/nginx/nginx.conf Наполняем:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 events {} http { server { listen 443 ssl; server_name confluence.domain.local; #Меняйте на своё имя ssl_certificate /etc/nginx/certs/selfsigned.crt; ssl_certificate_key /etc/nginx/certs/selfsigned.key; location / { proxy_pass http://confluence:8090; proxy_http_version 1.1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port 443; proxy_redirect off; } } } Далее создаём самоподписанные сертификаты:\n1 cd /opt/confluence/nginx 1 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout certs/selfsigned.key -out certs/selfsigned.crt -subj \u0026#34;/C=RU/ST=Moscow/L=My/O=ITZM/OU=TECH/CN=confluence.domain.local\u0026#34; Переходим в директорию с файлом:\n1 cd /opt/confluence/ И запускаем:\n1 docker-compose up -d Шаг 2. Настройка в веб-интерфейсе Переходим в браузер и набираем IP нашего хоста (Или DNS, если сделали запись)\nВыбираем Non-cluster (single node)\nДалее подключаемся к нашей базе данных, используя данные указанные в compose-файле\nПосле чего выйдет окно активации.\nШаг 3. Активация Процесс активации я подробно описал в Telegram.\nТам же отвечаю на вопросы и обновляю инструкции при необходимости\nПрисоединяйся Шаг 4. Завершаем настройку в веб-интерфейсе Создаёте тестовое пространство, тестовый проект и входите в Confluence.\nДальнейшее конфигурирование выполняется через панель администратора.\n","date":"2024-09-29T10:11:39+05:00","image":"https://itzm.tech/p/confluence-docker/confluence_hu6704600124542371647.jpg","permalink":"https://itzm.tech/p/confluence-docker/","title":"Установка Confluence на Ubuntu и Docker — пошаговая инструкция"},{"content":"У компании Atlassian есть несколько крутых продуктов, сегодня мы будем разворачивать Jira в Docker.\nJira — коммерческая система отслеживания ошибок, предназначена для организации взаимодействия с пользователями, хотя в некоторых случаях используется и для управления проектами.\nСамый быстрый способ развертывания данной системы, это использование докер контейнеров.\nТребования:\nСистема: Ubuntu 22.04 Установленный: Docker, docker-compose CPU: 4 vcpu RAM: 8 Gb HDD: На ваше усмотрение (Рекомендуется 100 GB, на 500+ пользователей) Шаг 1. Создаём директории 1 sudo mkdir -p /opt/jira/activate И файл:\n1 sudo nano /opt/jira/docker-compose.yaml Наполняем содержимым:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 version: \u0026#39;3.7\u0026#39; services: nginx: container_name: nginx image: nginx:alpine restart: always ports: - \u0026#34;443:443\u0026#34; volumes: - ./nginx/nginx.conf:/etc/nginx/nginx.conf - ./nginx/certs:/etc/nginx/certs:ro networks: - jira-proxy jira: container_name: jira image: atlassian/jira-software:9.12.9 restart: unless-stopped volumes: - var:/var/atlassian/application-data/jira - opt:/opt/atlassian/jira - ./activate:/opt/atlassian/atlassian-agent/ environment: - \u0026#39;JVM_MINIMUM_MEMORY=2048m\u0026#39; - \u0026#39;JVM_MAXIMUM_MEMORY=4096m\u0026#39; - \u0026#39;ATL_PROXY_NAME=jira.domain.local\u0026#39; - \u0026#39;ATL_PROXY_PORT=443\u0026#39; - \u0026#39;ATL_TOMCAT_SCHEME=https\u0026#39; - \u0026#39;TZ=Europe/Moscow\u0026#39; networks: - jira-proxy db: image: postgres:15-alpine environment: POSTGRES_USER: jiradb POSTGRES_PASSWORD: password POSTGRES_DB: jiradb PGDATA: /data/postgres volumes: - ./postgres:/data/postgres ports: - \u0026#34;5432:5432\u0026#34; restart: unless-stopped networks: - jira-proxy networks: jira-proxy: volumes: var: external: false opt: external: false Сохраняем, создаём директорию для nginx\n1 mkdir -p /opt/jira/nginx/certs И файл конфигурации:\n1 sudo nano /opt/jira/nginx/nginx.conf Наполняем:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 events {} http { server { listen 443 ssl; server_name jira.domain.local; #Меняйте на своё имя ssl_certificate /etc/nginx/certs/selfsigned.crt; ssl_certificate_key /etc/nginx/certs/selfsigned.key; location / { proxy_pass http://jira:8080; proxy_http_version 1.1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Port 443; proxy_redirect off; } } } Далее создаём самоподписанные сертификаты:\n1 cd /opt/jira/nginx 1 openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout certs/selfsigned.key -out certs/selfsigned.crt -subj \u0026#34;/C=RU/ST=Moscow/L=My/O=ITZM/OU=TECH/CN=jira.domain.local\u0026#34; Сохраняем, переходим в директорию с файлом и запускаем.\n1 2 cd /opt/jira/ docker-compose up -d Шаг 2. Настройка в веб-интерфейсе Переходим в браузер и набираем адрес нашего хоста, https://jira.domain.local\nМеняем язык на нужный вам и выбираем \u0026ldquo;Я выполню настройку самостоятельно\u0026rdquo;\nДалее \u0026ldquo;Моя база данных\u0026rdquo;\nВводим все данные из нашего контейнера db\nИ проводим \u0026ldquo;Тест подключения\u0026rdquo;\nНажимаем \u0026ldquo;Далее\u0026rdquo; и ждём пока создается база (Процесс может быть длительным ~5-20 минут)\nУказываем наш URL, по которому будет открываться Jira (с https)\nШаг 3. Активация Итак, мы подходим к моменту активации.\nПроцесс активации я подробно описал в Telegram.\nТам же отвечаю на вопросы и обновляю инструкции при необходимости\nПрисоединяйся Шаг 4. Завершаем настройку в веб-интерфейсе После активации вводим данные нашего локального admin\u0026rsquo;a JIRA\nНа данном этапе можно настроить получение уведомлений на email\nДалее идёт интуитивно понятная настройка используя кнопку \u0026ldquo;Далее\u0026rdquo;. Если при установке выбранный язык сбросится на дефолтный, то его можно указать в настройках сервера.\nСоздаете новый проект, открывается \u0026ldquo;Список задач\u0026rdquo;.\nУстановка завершена. Jira готова к работе.\nДополнения\nЕсли хотите, чтобы сообщение \u0026ldquo;Вы используете незаконную лицензию на продукт. Обратитесь к тому, кто вам ее предоставил.\u0026rdquo; Не отображалось, то на этапе активации продукта отключите интернет\u0026quot; Если нужно восстановить имеющуюся базу Jira из бэкапа, то положите её в контейнер по пути: /var/lib/docker/volumes/jira_var/_data/import/\nи скопируйте с заменой все директории (change, sentities, issues, snapshots, worklogs) в: /var/lib/docker/volumes/jira_var/_data/caches/indexesV2/ Восстановите бэкап через админскую панель в веб-интерфейсе Jira Если при переиндексации возникает ошибка проверьте разрешения на директории, должны быть: drwxr-x--- 2001:2001 ","date":"2024-09-18T19:11:39+05:00","image":"https://itzm.tech/p/jira-docker/jira_hu6924369108809654075.jpg","permalink":"https://itzm.tech/p/jira-docker/","title":"Установка Jira на Ubuntu и Docker — пошаговая инструкция"},{"content":"Иногда требуется выполнить массовое действие и быстрее всего это сделать через команды, чем использовать веб-интерфейс.\nВыгрузить список всех устройств, подключенных к каталогу FreeIPA:\n1 ipa host-find | grep \u0026#34;Host name:\u0026#34; Изменить количество вывода устройств в списке (500 устройств)\n1 ipa config-mod --searchrecordslimit=500 --searchtimelimit=5 Добавление хоста во FreeIPA. На клиенте выполняем команду:\n1 ipa-client-install --no-ntp --no-dns-sshfp --domain c --hostname=name.host.domain.local --mkhomedir --domain=DOMAIN.LOCAL --server=freeipa.domain.local Параметры: без встроенного ntp, dns. Указываем hostname клиента, создавать каталоги пользователей, домен, сервер подключения.\nПоиск пользователя:\n1 ipa user-find username Отключение пользователя:\n1 ipa user-disable username Добавление пользователя:\n1 ipa user-add username --first=User --last=Name --password Смена пароля пользователя:\n1 ipa user-mod username --password Добавить пользователя в группу:\n1 ipa group-add-member namegroup --users=username Информация о группе:\n1 ipa group-find namegroup Вывести все глобальные настройки:\n1 ipa config-show Вывод подробных настроек клиента:\n1 ipa host-show client.domain.local Вывод всех правил HBAC:\n1 ipa hbacrule-find --all --raw Вывод информации по конкретному правилу:\n1 ipa hbacrule-show --all --raw имя_правила Получить список пользователей из группы FreeIPA:\n1 ipa group-show --all --raw имя_группы Получить список хостов из группы:\n1 ipa hostgroup-show --all --raw имя_группы Пересоздать keytab файл FreeIPA\n1 ipa-getkeytab -s myipaexample.domain.local -p host/server-example.domain.local@DOMAIN.LOCAL -k /etc/krb5.keytab ","date":"2024-09-17T17:46:35+05:00","image":"https://itzm.tech/p/freeipa-commands/FreeIPACommands_hu16308151647380162295.jpg","permalink":"https://itzm.tech/p/freeipa-commands/","title":"Полезные команды FreeIPA"},{"content":"Когда пользователей в нашей FreeIPA \u0026ldquo;перевалило\u0026rdquo; за 50 человек возникла необходимость сделать оповещения о своевременной смене пароля. Благо есть встроенный дополнительный модуль, который позволяет это сделать. Имя этого модуля IPA-ENP, который \u0026ldquo;под капотом\u0026rdquo; представляет собой набор python скриптов.\nГлавный сервер FreeIPA у нас крутится в докер-компоузе. Заходим на сервер-клиента по ssh, который будет работать 24/7 и рассылать уведомления.\nШаг 1. Установка дополнительного модуля и настройка Получаем билет авторизации kerberos:\n1 kinit admin Устанавливаем дополнительный пакет:\n1 sudo apt install freeipa-client-epn Редактируем файл конфигурации, добавляя свои значения:\n1 sudo nano /etc/ipa/epn.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Адрес почтового сервера и порт smtp_server = server.mail.local smtp_port = 25 # Если авторизация на почтовом сервере через логин и пароль добавляем строки. # Если почтовый сервер принимает соединения без авторизации (по IP), то в этом нет необходимости. smtp_user = myuser@mail.local smtp_password = mypassword smtp_timeout = 60 smtp_security = none # Указываем email администратора. На этот адрес будут приходить письма, которые не дошли и уведомления. smtp_admin = admin@mail.local # Указываем от кого будут приходить сообщения. mail_from = myuser@mail.local #Указываем дни, когда будут приходить уведомления. За 14, 7, 3, 1 день до окончания пароля. notify_ttls = 14, 7, 3, 1 # Кодировка msg_charset = utf8 msg_subtype = plain Сохраняем и закрываем.\nШаг 2. Проверка конфигурации Узнать срок действия пароля пользователя:\n1 ipa user-show имя_пользователя --all --raw | grep krbPasswordExpiration Установить срок истечения пароля:\n1 ipa user-mod имя_пользователя --password-expiration=дата_истечения_срока_действия_пароля Где дата_истечения_срока_действия_пароля — строка вида: 20240910170748Z. Где, 2024 год 09 месяц 10 число 17 часы 07 минуты 48 секунды\nДля проверки можем сменить пароль пользователю:\n1 ipa user-mod имя_пользователя --setattr=krbPasswordExpiration=20240911170748Z Получается мы попадаем под условие, что до истечения пароля пользователя остаётся 1 день и ему нужно отправить сообщение. Для теста я создал в FreeIPA пользователя с именем user, и сменил ему срок истечения пароля командами выше.\nТестируем нашу конфигурацию без отправки сообщения.\n1 sudo ipa-epn --dry-run Получаем вывод:\n1 2 3 4 5 6 7 8 9 10 11 [ { \u0026#34;uid\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;cn\u0026#34;: \u0026#34;user test\u0026#34;, \u0026#34;givenname\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;sn\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;krbpasswordexpiration\u0026#34;: \u0026#34;2024-09-10 17:07:48\u0026#34;, \u0026#34;mail\u0026#34;: \u0026#34;[\u0026#39;user@mail.local\u0026#39;]\u0026#34; } ] The IPA-EPN command was successful Шаг 3. Настройка текста сообщения Далее настраиваем шаблон сообщения в файле, который будет приходить пользователю.\n1 sudo nano /etc/ipa/epn/expire_msg.template В моём случае я удалил все строки и добавил:\nДобрый день, {fullname}!\nПросим Вас сменить пароль от учетной записи {uid}, который действует до {expiration}. Во избежании блокировок измените пароль от FreeIPA заранее.\nМы можем использовать переменные, для шаблона сообщения: 1 2 3 4 5 ID пользователя: uid Полное имя: fullname Имя: first Фамилия: last Срок действия пароля: expiration Сохраняем, закрываем.\nШаг 4. Отправляем сообщение Тестируем отправку сообщения администратору, которого мы указали в конфиге smtp_admin = admin@mail.local. Пользователям это сообщение не дойдёт, так как мы указали параметр -–mail-test.\n1 sudo ipa-epn -–mail-test Отправляем сообщения пользователям:\n1 sudo ipa-epn Смотрим на шаблон и вид сообщения. Если все устраивает, то запускаем встроенный таймер, который будет автоматически делать запрос во FreeIPA и отправлять уведомления. По умолчанию он запускается каждую ночь в 01:00 часов.\n1 2 sudo systemctl start ipa-epn.timer sudo systemctl enable ipa-epn.timer Если время нужно изменить время отправки, допустим в 08:00, то отредактируем параметр сервиса:\n1 sudo nano /usr/lib/systemd/system/ipa-epn.timer 1 2 3 [Timer] OnCalendar= OnCalendar=*-*-* 08:00:00 Сохраняем, выходим.\nПроверяем сервис:\n1 2 sudo systemctl show ipa-epn.timer | grep OnCalendar TimersCalendar={ OnCalendar=*-*-* 08:00:00 ; next_elapse=n/a } Настройка завершена. Даже если север в дальнейшем перезапуститься наша служба timer включится и выполнит задание по расписанию.\n","date":"2024-09-16T23:34:43+05:00","image":"https://itzm.tech/p/freeipa-password/FreeIPAPassword_hu8828558613834299715.jpg","permalink":"https://itzm.tech/p/freeipa-password/","title":"FreeIPA. Отправка уведомления пользователю об истечении пароля"},{"content":"Появилась необходимость сделать отказоустойчивый кластер БД, так как Patroni уже используется и давно в качестве проксирования запросов выбор пал на HAProxy.\nHAProxy установим в качестве контейнера в docker-compose\nIP: 192.168.10.9 Сервера Patroni:\nIP: 192.168.10.10 IP: 192.168.10.11 Конфигурирование HAProxy состоит из 3 основных частей:\nСекция global параметры здесь устанавливаются для всего конфига Аргументы из командной строки имеют наивысший приоритет В блоках defaults, listen, frontend, backend указываем настройки проксирования запросов Приступим.\nШаг 1. Конфигурирование сервера Создаем директории:\n1 sudo mkdir -p /opt/haproxy/config Создаем файл:\n1 sudo touch /opt/haproxy/config/haproxy.cfg Открываем на редактирование haproxy.cfg\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 global maxconn 4096 defaults mode http timeout connect 5000 timeout client 50000 timeout server 50000 listen stats bind *:8443 mode tcp stats enable stats uri /stats stats refresh 10s frontend balancer bind *:5432 mode tcp default_backend web_backends backend web_backends mode tcp option httpchk http-check expect status 200 balance roundrobin default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions server serv1 192.168.9.228:5432 maxconn 100 check port 8008 server serv2 192.168.9.229:5432 maxconn 100 check port 8008 Описание блоков настроек:\ndefaults - параметры по умолчанию для всех блоков, которые находятся ниже. listen stats - настройки страницы статистики. frontend - указываем где будут приниматься клиентские запросы. backend - указываем сервера Patroni, на которые будут перенаправляться запросы. В секции backend указано:\n1 default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions Если выполнено 3 попытки с интервалом по 3 секунды и сервер не ответил, то соединение будет прервано и переключено на другой сервер.\nШаг 2. Запуск HAProxy в docker-compose Создаём docker-compose файл с образом HAProxy:\n1 sudo touch /opt/haproxy/docker-compose.yaml Содержимое файла:\n1 2 3 4 5 6 7 8 9 version: \u0026#34;3.9\u0026#34; services: haproxy: image: haproxy:2.8.0 volumes: - ./config/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro ports: - \u0026#34;8443:8443\u0026#34; - \u0026#34;5432:5432\u0026#34; Сохраняем и запускаем:\n1 2 cd /opt/haproxy sudo docker-compose up -d Заходим в веб-интерфейс и проверяем статусы наших серверов Patroni\n1 http://192.168.10.9:8443 Установка и настройка завершена.\n","date":"2024-08-23T18:39:42+05:00","image":"https://itzm.tech/p/haproxy/haproxy_hu16625306303711626206.jpg","permalink":"https://itzm.tech/p/haproxy/","title":"High Availability Cluster HAProxy + Patroni"},{"content":"Шаг 1. Установка Ansible-роли В консоли сервера FreeIPA выполняем обновление пакетов:\n1 sudo apt update -y \u0026amp;\u0026amp; apt upgrade -y Загружаем на наш доменный сервер FreeIPA ansible и ansible роль:\n1 2 3 sudo apt install ansible -y sudo ansible-galaxy install ipaclient sudo apt install sshpass -y Создаем и переходим в рабочую директорию:\n1 sudo mkdir /opt/freeipa \u0026amp;\u0026amp; cd /opt/freeipa Шаг 2. Создаем файл inventory.ini\nПрописываем:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Сервера-клиенты на присоединение в домен: [ipaclients] client-1.domain.local client-2.domain.local client-3.domain.local # Сервер домена FreeIPA: [ipaservers] freeipa.domain.local # Переменные для мастера установки (меняем на свои): [ipaclients:vars] ipaclient_domain=domain.local ipaclient_realm=DOMAIN.LOCAL ipaadmin_principal=admin ipaadmin_password=changeMe ipaclient_no_ntp=yes ipaclient_mkhomedir=yes Сохраняем, закрываем.\nВсе сервера в списках должны резолвиться по имени.\nДополнительно в этой же директории можно создать файл ansible.cfg\n1 2 [defaults] host_key_checking=false Чтобы при подключении сервер не запрашивал отпечаток ключа.\nЗдесь же создаём файл ipa-playbook.yaml\n1 nano ipa-playbook.yaml Содержимое которого (remote_user меняйте на своего пользователя удаленного компьютера, который имеет право выполнять команды под sudo):\n1 2 3 4 5 6 7 8 9 10 11 --- - name: \u0026#34;Playbook to configure IPA clients\u0026#34; hosts: ipaclients become: true remote_user: admin gather_facts: true roles: - role: ipaclient state: present ... Также, возможно необходимо будет доустановить библиотеку python 1 2 sudo apt install python3-pip sudo pip install python-gssapi Запускаем ansible playbook:\n1 ansible-playbook ipa-playbook.yaml -i inventory.ini Дожидаемся процесса установки. Готово. Наши сервера успешно присоединены к домену.\n","date":"2024-08-20T00:08:15+05:00","image":"https://itzm.tech/p/freeipa-client-ansible/FreeIPAAnsible_hu621641454013338979.jpg","permalink":"https://itzm.tech/p/freeipa-client-ansible/","title":"Добавление клиентов FreeIPA используя Ansible"},{"content":"Шаг 1. Подготовка клиента Для начала перейдем на хост и выполним стандартную процедуру обновления пакетов:\n1 sudo apt update -y \u0026amp;\u0026amp; apt upgrade -y Затем меняем hostname, где после первой точки указываем наш домен FreeIPA:\n1 sudo hostnamectl set-hostname client1.domain.local Перелогиниваемся под нашим пользователем, в моем случае user:\n1 su user Изменяем сетевые настройки, в частности указываем в качестве DNS сервера IP адрес нашего FreeIPA сервера:\n1 sudo nano /etc/netplan/00-installer-config.yaml Прописываем:\n1 2 3 4 5 6 7 8 9 10 11 12 13 network: ethernets: ens33: dhcp4: false addresses: - 192.168.10.177/24 routes: - to: default via: 192.168.10.2 nameservers: addresses: - 192.168.10.188 - 8.8.8.8 Где:\nens33 – название вашего сетевого интерфейса (Узнать командой ip a) 192.168.10.177 – адрес нашего текущего сервера client1.domain.local 192.168.10.2 – Шлюз 192.168.10.188 – DNS адрес (IP нашего FreeIPA сервера) 8.8.8.8 – Резервный DNS гугла Сохраняем, применяем\n1 sudo netplan apply Дополнительно сервер можно жестко забить в hosts\n1 sudo nano /etc/hosts Добавить строчку:\n192.168.10.188 freeipa freeipa.domain.local\nПингуем наш сервер каталога:\n1 ping freeipa.domain.local Шаг 2. Установка клиента Устанавливаем пакет для клиента FreeIPA:\n1 sudo apt install freeipa-client На все вопросы нажимаем Enter.\nШаг 3. Присоединение клиента к домену Все готово для присоединения нашего клиента к домену FreeIPA. Выполняем команду:\n1 sudo ipa-client-install --mkhomedir --domain domain.local --realm DOMAIN.LOCAL --enable-dns-updates Где:\ndomain.local DOMAIN.LOCAL Ваш домен FreeIPA.\nОтвечаем на вопросы установщика:\n1 2 Provide your IPA server name (ex: ipa.example.com): freeipa.domain.local Proceed with fixed values and no DNS discovery? [no]: yes Далее Enter\n1 Continue to configure the system with these values? [no]: yes Далее вводим логин и пароль администратора домена:\n1 2 User authorized to enroll computers: admin Password for admin@DOMAIN.LOCAL: После завершения процесса наш компьютер успешно присоединен к домену. Это можно увидеть в веб-интерфейсе\n","date":"2024-08-20T00:08:15+05:00","image":"https://itzm.tech/p/freeipa-client/FreeIPAClients_hu5068296053694258946.jpg","permalink":"https://itzm.tech/p/freeipa-client/","title":"Добавление клиентов FreeIPA. Ручной метод"},{"content":"Установка будет происходить в 3 этапа:\nВключение дополнительной роли на сервере AD Создание и назначение групповой политики Проверка на конечном устройстве Для начала включаем модуль для хранения ключей дешифрования на сервере Active Directory. Диспетчер серверов Добавить роли и компоненты\nУстановка ролей или компонентов - Далее\nВыбираем наш сервер (Отмечен по умолчанию)\nРоли сервера - нажимаем далее\nКомпоненты отмечаем галкой \u0026ldquo;Шифрование диска BitLocker\u0026rdquo;\nОтмечаем галками как на скриншоте и нажимаем \u0026ldquo;Добавить компоненты\u0026rdquo;\nУстанавливаем галку \u0026ldquo;Автоматический перезапуск конечного сервера\u0026rdquo; и нажимаем \u0026ldquo;Установить\u0026rdquo;\nСоздаём OU-шку для компьютеров, на которые будем раскатывать GPO\nЗакидываем наш компьютер\nСоздаём групповую политику Переходим в Управление групповой политикой На созданной нами OU - Создать объект групповой политики и связать его Называем политику, в моём случае Шифрование\nИзменить\nПереходим в \u0026ldquo;Конфигурация компьютера\u0026rdquo; - \u0026ldquo;Политики\u0026rdquo; - \u0026ldquo;Административные шаблоны\u0026rdquo; - Компоненты Windows - Шифрование диска\nУстанавливаем параметры в \u0026ldquo;Включено\u0026rdquo; и отмечаем галками\nПереходим в подгруппу \u0026ldquo;Диски операционной системы\u0026rdquo;\nПереходим в подгруппу \u0026ldquo;Диски операционной системы\u0026rdquo;\nВключаем \u0026ldquo;Выбор методов восстановления дисков операционной системы..\u0026rdquo; Отмечаем всё галками, если ключ планируется дополнительно хранить в виде файла или на внешнем накопителе убрать пункт \u0026ldquo;Пропускать параметры восстановления в мастере установки Bitlocker\u0026rdquo;\nШифрование на конечном устройстве Ручной способ: Переходим на ПК с именем New в Компьютер - Локальный диск С - Свойства - \u0026ldquo;Включить битлокер\u0026rdquo;\nВыбираем \u0026ldquo;Шифровать только занятое место\u0026rdquo;\nСтавим галку \u0026ldquo;Запустить проверку систему BitLocker\u0026rdquo;\nДалее компьютер перезагрузится и начнётся процесс шифрования (Посмотреть статус выполнения можно в трее)\nАвтоматизировать данную операцию можно командой Powershell:\n1 PS C:\\Users\\admin\u0026gt; manage-bde -on C: -UsedSpaceOnly -SkipHardwareTest -recoverypassword Ключи восстановления можно найти в Active Directory\nПри необходимости можно нацелить политику на отдельные компьютеры, например если они распределены по разным OU. Для этого в фильтрах безопасности политики \u0026ldquo;Шифрование\u0026rdquo; удаляем \u0026ldquo;Прошедшие проверку\u0026rdquo; Нажимаем добавить - Тип объектов - Ставим галочку \u0026ldquo;Компьютер\u0026rdquo;\nВводим имя нашего ПК и нажимаем добавить\nГотово! Теперь компьютер зашифрован, при установке носителя в другое устройство будет запрошен ключ дешифрования. Сейчас дешифрование происходит без необходимости пользователем вводить пароль. Основано на чипе TPM, встроенного в наш компьютер. Ключ восстановления хранится в Active Directory. При запросе на дешифрование устройство выведет первые 8 символов ключа, с помощью которых можно выполнить поиск, найти компьютер и полный ключ восстановления.\n","date":"2024-08-12T22:53:39+05:00","image":"https://itzm.tech/p/bitlocker/bitlocker_hu3769065890617136874.jpg","permalink":"https://itzm.tech/p/bitlocker/","title":"Bitlocker + Recovery keys Active Directory"},{"content":"Установка выполняется на Ubuntu 22.04\nПредполагается, что у вас уже установлен docker, docker-compose и ansible.\nШаг 1. Настройка сервера Назначаем имя серверу:\n1 sudo hostnamectl set-hostname freeipa.domain.local Далее скачиваем ansible-role с официального сайта:\n1 sudo ansible-galaxy role install ipaclient Если получаем ошибку «ERROR! Unexpected Exception, this is probably a bug: No module named \u0026lsquo;gssapi\u0026rsquo; Надо доустановить:\n1 sudo apt install krb5-config libkrb5-dev И запустить установку через pip:\n1 /usr/bin/pip install gssapi –pre Настраиваем параметры системного ядра, чтобы избежать ошибки \u0026ldquo;Failed to create /init.scope control group: No such file or directory\u0026rdquo;\n1 2 3 echo \u0026#39;GRUB_CMDLINE_LINUX=systemd.unified_cgroup_hierarchy=false\u0026#39; \u0026gt; /etc/default/grub.d/cgroup.cfg sudo update-grub sudo reboot Шаг 2. Запускаем freeipa в контейнере Создаем директорию freeipa в /opt и docker-compose файл в opt/freeipa:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 --- version: \u0026#34;3.8\u0026#34; services: freeipa: image: freeipa/freeipa-server:rocky-8 hostname: freeipa domainname: domain.local container_name: freeipa ports: - 80:80 - 443:443 - 389:389 - 636:636 - 88:88 - 464:464 - 88:88/udp - 464:464/udp - 123:123/udp dns: - 192.168.10.190 restart: unless-stopped tty: true stdin_open: true environment: IPA_SERVER_HOSTNAME: freeipa.domain.local IPA_SERVER_IP: 192.168.10.188 TZ: \u0026#34;Europe/Moscow\u0026#34; command: - ipa-server-install - --domain=domain.local - --realm=DOMAIN.LOCAL - --admin-password=changeME - --http-pin=mypass - --dirsrv-pin=mypass - --ds-password=changeME - --no-host-dns - --setup-dns - --auto-forwarders - --allow-zone-overlap - --no-dnssec-validation - --unattended cap_add: - SYS_TIME volumes: - /sys/fs/cgroup:/sys/fs/cgroup - ./data:/data - ./logs:/var/logs sysctls: - net.ipv6.conf.all.disable_ipv6=0 ... Запускаем:\n1 docker-compose up -d Далее переходим в веб-консоль по нашему адресу:\nЗдесь нажимаем \u0026ldquo;Отмена\u0026rdquo;\nАвторизуемся:\n1 2 Логин: admin Пароль: changeME (Указали в docker-compose файле, параметр --admin-password) Наш домен FreeIPA создан. Дальнейшее конфигурирование выполняется в панели администратора веб-интерфейса.\n","date":"2024-08-11T23:01:42+05:00","image":"https://itzm.tech/p/freeipa/freeIPA_hu1332689740339089341.jpg","permalink":"https://itzm.tech/p/freeipa/","title":"Установка сервера FreeIPA в Docker-compose"}]